{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "version": "3.8.8", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Water consumption", "metadata": {}}, {"cell_type": "markdown", "source": "---", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:25:24.015057Z", "start_time": "2021-04-07T09:25:24.010199Z"}}}, {"cell_type": "markdown", "source": "**Version:** Performance analysis\n\n*(Sergi Domingo sergi.domingo@urv.cat)*\n\nCheck available KPIs at:\n\n- [Interpolation](#KPIs-(Interpolation))\n- [Gather blocks](#KPIs-(Gather-blocks))\n- [Potential evaporation](#KPIs-(Potential-evaporation))\n\n**Summary of a sample execution**:\n\n- Interpolation\n    - The whole interpolation process (map interpolation + radiation interpolation) was distributed across 288 Cloud Functions (each allocated 2048MB).\n    - The computational process for ~2GiB of .TIFF map data lasted around ~4 minutes. \n    - Throughput was 7.2295 MiB/s.\n    - The process cost ~0.30 dollars.\n    - Speedup for a part of the process was computed and the results showed excellent scalability (90% of the ideal speedup) for experiments under 100 parallel Cloud Functions, compared to a base case.\n    \n    \n- Gather blocks\n    - This quick process deployed 40 Cloud Functions (each allocated 2048MB) to join the ~2GiB of independent chunk data into bigger blocks.\n    - Throughput was 60.1322 MiB/s.\n    - The process lasted for ~30s and cost ~0.02 dollars.\n   \n   \n- Potential evaporation\n    - Lithops divided the potential evaporation process into 8 Cloud Functions.\n    - The process allocated 16GiB of memory and computed ~6GiB of .TIFF files in ~3 minutes.\n    - The cost of the process was ~0.03 dollars.\n    - Throughput was 33.9837 MiB/s.\n    - Speedup compared to a base case of 2 workers was above 92% for experiments run with 4 and 8 workers.    ", "metadata": {}}, {"cell_type": "code", "source": "from IPython.display import Image\nimport matplotlib.pyplot as plt", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:23.745397Z", "start_time": "2021-04-13T14:38:23.312419Z"}, "trusted": true}, "execution_count": 1, "outputs": []}, {"cell_type": "code", "source": "def get_process_cost(lith):\n    import pandas as pd\n    df = pd.read_csv(lith.log_path)\n    cost = float(df[df[\"Job_ID\"] == \"Summary\"][\"Cost\"])\n    return cost", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:24.181256Z", "start_time": "2021-04-13T14:38:24.175102Z"}, "trusted": true}, "execution_count": 2, "outputs": []}, {"cell_type": "markdown", "source": "---", "metadata": {}}, {"cell_type": "markdown", "source": "The current notebook computes an interpolation of temperatures in each pixel based on SIAM extracted data.", "metadata": {}}, {"cell_type": "markdown", "source": "A .bluemix/cos_credentials.json file correctly configured located at home directory is needed in order to connect with IBM Cloud. More information at https://cloud.ibm.com/docs/services/cloud-object-storage/iam?topic=cloud-object-storage-service-credentials.", "metadata": {}}, {"cell_type": "code", "source": "from collections import defaultdict\nfrom ibm_botocore.client import Config, ClientError\nfrom geospatial_usecase.io_utils.cos import COS\nfrom geospatial_usecase.io_utils.plot import plot_random_blocks, plot_results\nfrom rasterio.windows import Window\nfrom scipy.spatial import distance_matrix\nfrom shapely.geometry import Point, MultiPoint, box\nfrom pprint import pprint\n\nimport os\nimport ibm_boto3\nimport math\nimport numpy as np\nimport pandas as pd\nimport lithops\nimport requests\nimport rasterio\nimport json\nimport random\n\nfrom lithops.storage import Storage", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:27.706148Z", "start_time": "2021-04-13T14:38:27.027515Z"}, "trusted": true}, "execution_count": 3, "outputs": []}, {"cell_type": "markdown", "source": "## Global parameters", "metadata": {}}, {"cell_type": "markdown", "source": "Area outside the processed tile that we want to consider for taking SIAM stations into account:", "metadata": {}}, {"cell_type": "code", "source": "AREA_OF_INFLUENCE = 4000", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:28.795793Z", "start_time": "2021-04-13T14:38:28.788173Z"}, "trusted": true}, "execution_count": 4, "outputs": []}, {"cell_type": "markdown", "source": "IBM Cos bucket to upload files:", "metadata": {}}, {"cell_type": "code", "source": "BUCKET = 'geospatial-usecase'", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:29.658886Z", "start_time": "2021-04-13T14:38:29.654251Z"}, "trusted": true}, "execution_count": 5, "outputs": []}, {"cell_type": "markdown", "source": "Runtime name", "metadata": {}}, {"cell_type": "code", "source": "RUNTIME = 'tfmurv/lithops-k8s-geospatial'\n# RUNTIME = 'jsampe/lithops-k8s-geospatial'", "metadata": {"trusted": true}, "execution_count": 6, "outputs": []}, {"cell_type": "markdown", "source": "Split tile into SPLITS$^2$ chunks:", "metadata": {}}, {"cell_type": "code", "source": "SPLITS = 3", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:30.525082Z", "start_time": "2021-04-13T14:38:30.519883Z"}, "trusted": true}, "execution_count": 7, "outputs": []}, {"cell_type": "markdown", "source": "Correlation coefficient between elevation and temperature:", "metadata": {}}, {"cell_type": "code", "source": "r = -0.0056", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:31.362634Z", "start_time": "2021-04-13T14:38:31.359578Z"}, "trusted": true}, "execution_count": 8, "outputs": []}, {"cell_type": "markdown", "source": "Elevation to interpolate temperature:", "metadata": {}}, {"cell_type": "code", "source": "zdet = 2000", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:32.187402Z", "start_time": "2021-04-13T14:38:32.184798Z"}, "trusted": true}, "execution_count": 9, "outputs": []}, {"cell_type": "markdown", "source": "Day of year to calculate solar irradiation:", "metadata": {}}, {"cell_type": "code", "source": "DAY_OF_YEAR = 50", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:33.304420Z", "start_time": "2021-04-13T14:38:33.299098Z"}, "trusted": true}, "execution_count": 10, "outputs": []}, {"cell_type": "markdown", "source": "Object storage key prefix, to keep objects organized:", "metadata": {}}, {"cell_type": "code", "source": "cloud_storage = Storage(backend=\"ceph\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:34.978991Z", "start_time": "2021-04-13T14:38:34.905888Z"}, "trusted": true}, "execution_count": 11, "outputs": []}, {"cell_type": "markdown", "source": "## Data preparation", "metadata": {}}, {"cell_type": "markdown", "source": "This section fetches and uploads to COS the metadata used in the workflow. It can be skipped if the data is already in COS.", "metadata": {}}, {"cell_type": "markdown", "source": "### SIAM data", "metadata": {}}, {"cell_type": "code", "source": "url = 'http://siam.imida.es/apex/f?p=101:47:493289053024037:CSV::::'\n# url = 'http://siam.imida.es/apex/f?p=101:48:2555846978143339:CSV::::'\nsiam_data = requests.get(url)\nwith open('geospatial_usecase/siam_data.csv', 'wb') as siam_data_file:\n    siam_data_file.write(siam_data.content)", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:42:03.976592Z", "start_time": "2021-04-07T09:42:02.944388Z"}, "tags": [], "trusted": true}, "execution_count": 12, "outputs": []}, {"cell_type": "code", "source": "def guess_nearest(x, y, field, stations):\n    '''\n    Compute field value at a given x,y point by getting the value of the closest station \n    '''\n    from shapely.ops import nearest_points\n    stations_of_interest = stations[(stations[field] != '-') & ((stations['X'] != x) | (stations['Y'] != y))]\n    points = MultiPoint(stations_of_interest.apply(lambda row: Point(row['X'], row['Y']), axis=1 ).array)\n    nearest = nearest_points(Point(x,y), points)[1]\n    val = stations_of_interest[(stations_of_interest['X'] == nearest.x) &\n                                (stations_of_interest['Y'] == nearest.y)]\n\n    return stations_of_interest[(stations_of_interest['X'] == nearest.x) &\n                                (stations_of_interest['Y'] == nearest.y)][field].iloc[0] ", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:42:06.892586Z", "start_time": "2021-04-07T09:42:06.886566Z"}, "trusted": true}, "execution_count": 13, "outputs": []}, {"cell_type": "markdown", "source": "Append locations of SIAM stations to previously downloaded data and write results to a CSV:", "metadata": {}}, {"cell_type": "code", "source": "columns = {\n    'Estaci\u00f3n': 'COD',\n    'Tmed <br> (\u00baC)': 'temp',\n    'Hrmed <br> (%)': 'hr',\n    'Vvmed <br> (m/seg)': 'v',\n    'Eti.': 'dir',\n    'Radmed <br> (w/m2)': 'rad',\n    'Dvmed <br>  (\u00ba)': 'dir_deg'\n}\n\nsiam_data = pd.read_csv('geospatial_usecase/siam_data.csv', encoding='iso-8859-1',\n                        sep=';', decimal=',', thousands='.', na_values = '-')\nsiam_data = siam_data[columns.keys()].rename(columns=columns)\nsiam_locations = pd.read_csv('geospatial_usecase/siam_locations.csv', encoding='iso-8859-1', sep=';', decimal = ',', thousands='.')\nsiam = pd.merge(siam_locations, siam_data, on='COD')\nsiam['tdet'] = siam['temp'] + r * (zdet - siam['Cota'].to_numpy())\nsiam = siam[['X', 'Y', 'Cota', 'temp', 'hr', 'tdet', 'v'] + list(columns.values())]\n# Guess wind direction of undefined values\nsiam['dir_deg'] = siam.apply(lambda row: row['dir_deg'] \n                                     if not math.isnan(row['dir_deg'])\n                                     else guess_nearest(row['X'], row['Y'], 'dir_deg', siam), axis=1)\n# Guess radiation of undefined values\nsiam['rad'] = siam.apply(lambda row: row['rad'] \n                                     if not math.isnan(row['rad'])\n                                     else guess_nearest(row['X'], row['Y'], 'rad', siam), axis=1)\nsiam.to_csv('geospatial_usecase/siam.csv', index=False)", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:42:12.829971Z", "start_time": "2021-04-07T09:42:12.755771Z"}, "trusted": true}, "execution_count": 14, "outputs": [{"name": "stderr", "text": "/opt/conda/lib/python3.8/site-packages/pandas/core/ops/__init__.py:1115: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  result = method(y)\n", "output_type": "stream"}]}, {"cell_type": "markdown", "source": "Upload the output CSV to COS:", "metadata": {}}, {"cell_type": "code", "source": "with open('geospatial_usecase/siam.csv', 'rb') as siam_out_file:\n    cloud_storage.put_object(bucket=BUCKET, key='siam.csv', body=siam_out_file)", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:46:49.948481Z", "start_time": "2021-04-07T09:46:49.800147Z"}, "trusted": true}, "execution_count": 15, "outputs": []}, {"cell_type": "markdown", "source": "### MDT (Modelo digital del terreno) data", "metadata": {}}, {"cell_type": "markdown", "source": "Download MDT files for free from http://centrodedescargas.cnig.es/CentroDescargas/buscadorCatalogo.do?codFamilia=MDT05# and put them in `MDT` folder.", "metadata": {}}, {"cell_type": "markdown", "source": "Find downloaded MDTs:", "metadata": {}}, {"cell_type": "code", "source": "mdt_folder = 'geospatial_usecase/MDT'\nmdts = [os.path.join(mdt_folder, mdt) for mdt in os.listdir(mdt_folder) if mdt.endswith('.asc')]\nmdts", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:48:12.000399Z", "start_time": "2021-04-07T09:48:11.986192Z"}, "trusted": true}, "execution_count": 16, "outputs": [{"execution_count": 16, "output_type": "execute_result", "data": {"text/plain": "[]"}, "metadata": {}}]}, {"cell_type": "code", "source": "tiles = [os.path.splitext(os.path.basename(mdt))[0] for mdt in mdts]\ntiles", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:48:13.697506Z", "start_time": "2021-04-07T09:48:13.691877Z"}, "trusted": true}, "execution_count": 17, "outputs": [{"execution_count": 17, "output_type": "execute_result", "data": {"text/plain": "[]"}, "metadata": {}}]}, {"cell_type": "markdown", "source": "Convert digital elevation map into a Cloud Optimized Geotiff. Upload then to IBM COS:", "metadata": {}}, {"cell_type": "code", "source": "for mdt in mdts:\n    tiff_file = os.path.splitext(mdt)[0] + '.tif'\n    with rasterio.open(mdt) as src:\n        profile = src.profile\n        # Cloud optimized GeoTiff parameters (No hace falta rio_cogeo)\n        profile.update(driver='GTiff')\n        profile.update(blockxsize=256)\n        profile.update(blockysize=256)\n        profile.update(tiled=True)\n        profile.update(compress='deflate')\n        profile.update(interleave='band')\n        with rasterio.open(tiff_file, \"w\", **profile) as dest:\n            dest.write(src.read())", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:49:39.303588Z", "start_time": "2021-04-07T09:48:35.778105Z"}, "trusted": true}, "execution_count": 18, "outputs": []}, {"cell_type": "code", "source": "mdts_gtiff = [os.path.join(mdt_folder, mdt) for mdt in os.listdir(mdt_folder) if mdt.endswith('.tif')]\nmdts_gtiff", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:47.839384Z", "start_time": "2021-04-13T14:38:47.833205Z"}, "trusted": true}, "execution_count": 19, "outputs": [{"execution_count": 19, "output_type": "execute_result", "data": {"text/plain": "[]"}, "metadata": {}}]}, {"cell_type": "code", "source": "for mdt_gtiff in mdts_gtiff:\n    with open(mdt_gtiff, 'rb') as mdt_file:\n        cloud_storage.put_object(bucket=BUCKET, key=mdt_gtiff, body=mdt_file)", "metadata": {"ExecuteTime": {"end_time": "2021-04-07T09:51:19.779414Z", "start_time": "2021-04-07T09:50:38.654162Z"}, "trusted": true}, "execution_count": 20, "outputs": []}, {"cell_type": "markdown", "source": "## Serverless computation", "metadata": {}}, {"cell_type": "markdown", "source": "Input MDT tiles to process:", "metadata": {}}, {"cell_type": "code", "source": "tiles = [os.path.splitext(os.path.basename(key))[0]\n         for key in cloud_storage.list_keys(bucket=BUCKET, prefix='MDT')]\ntiles", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:50.635461Z", "start_time": "2021-04-13T14:38:50.327866Z"}, "trusted": true}, "execution_count": 21, "outputs": [{"execution_count": 21, "output_type": "execute_result", "data": {"text/plain": "['PNOA_MDT05_ETRS89_HU30_0891_LID',\n 'PNOA_MDT05_ETRS89_HU30_0892_LID',\n 'PNOA_MDT05_ETRS89_HU30_0912_LID',\n 'PNOA_MDT05_ETRS89_HU30_0913_LID',\n 'PNOA_MDT05_ETRS89_HU30_0933_LID',\n 'PNOA_MDT05_ETRS89_HU30_0934_LID',\n 'PNOA_MDT05_ETRS89_HU30_0954_LID',\n 'PNOA_MDT05_ETRS89_HU30_0955_LID']"}, "metadata": {}}]}, {"cell_type": "code", "source": "# DEBUG\n# tiles = ['PNOA_MDT05_ETRS89_HU30_0933_LID']\n# tiles", "metadata": {"ExecuteTime": {"end_time": "2021-03-29T10:06:07.523375Z", "start_time": "2021-03-29T10:05:14.468Z"}, "trusted": true}, "execution_count": 22, "outputs": []}, {"cell_type": "markdown", "source": "Compute solar irradiation given a day of year using GRASS libraries:", "metadata": {}}, {"cell_type": "code", "source": "def compute_solar_irradiation(inputFile, outputFile, crs='32630'):\n    # Define grass working set\n    GRASS_GISDB = 'grassdata'\n    GRASS_LOCATION = 'GEOPROCESSING'\n    GRASS_MAPSET = 'PERMANENT'\n    GRASS_ELEVATIONS_FILENAME = 'ELEVATIONS'\n    \n    import os\n    import shutil\n    os.environ['GRASSBIN'] = 'grass76'\n    from grass_session import Session\n    import grass.script as gscript\n    from grass.pygrass.modules.shortcuts import general as g\n    from grass.pygrass.modules.shortcuts import raster as r\n    import re\n    os.environ.update(dict(GRASS_COMPRESS_NULLS='1'))\n    \n    # Clean previously processed data\n    if os.path.isdir(GRASS_GISDB):\n        shutil.rmtree(GRASS_GISDB)\n    with Session(gisdb=GRASS_GISDB, location=GRASS_LOCATION, mapset=GRASS_MAPSET, create_opts='EPSG:32630') as ses:\n    \n        # Set project projection to match elevation raster projection\n        g.proj(epsg=crs, flags='c') \n    \n        # Load raster file into working directory\n        r.import_(input=inputFile, \n                  output=GRASS_ELEVATIONS_FILENAME, \n                  flags='o')    \n        \n        # Set project region to match raster region\n        g.region(raster=GRASS_ELEVATIONS_FILENAME, flags='s')    \n        # Calculate solar irradiation\n        gscript.run_command('r.slope.aspect', elevation=GRASS_ELEVATIONS_FILENAME,\n                            slope='slope', aspect='aspect')\n        gscript.run_command('r.sun', elevation=GRASS_ELEVATIONS_FILENAME,\n                            slope='slope', aspect='aspect', beam_rad='beam',\n                            step=1, day=DAY_OF_YEAR)\n        \n        # Get extraterrestrial irradiation from history metadata\n        regex = re.compile(r'\\d+\\.\\d+')\n        output = gscript.read_command(\"r.info\", flags=\"h\", map=[\"beam\"])\n        splits = str(output).split('\\n')\n        line = next(filter(lambda line: 'Extraterrestrial' in line, splits))\n        extraterrestrial_irradiance = float(regex.search(line)[0])\n        \n        # Export generated results into a GeoTiff file\n        if os.path.isfile(outputFile):\n            os.remove(outputFile)\n            \n        r.out_gdal(input='beam', output=outputFile)\n        \n        return extraterrestrial_irradiance", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:54.462039Z", "start_time": "2021-04-13T14:38:54.449706Z"}, "tags": [], "trusted": true}, "execution_count": 23, "outputs": []}, {"cell_type": "markdown", "source": "Get stations contained in the area of interest:", "metadata": {}}, {"cell_type": "code", "source": "def filter_stations(bounds, stations):\n    total_points = MultiPoint([Point(x,y) for x, y in stations[['X', 'Y']].to_numpy()])\n    intersection = bounds.buffer(AREA_OF_INFLUENCE).intersection(total_points)\n    \n    return stations[[ intersection.contains(point) for point in total_points]]", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:55.461635Z", "start_time": "2021-04-13T14:38:55.457230Z"}, "trusted": true}, "execution_count": 24, "outputs": []}, {"cell_type": "markdown", "source": "Inverse Distance Weighting interpolation:", "metadata": {}}, {"cell_type": "code", "source": "def compute_basic_interpolation(shape, stations, field_value, offset = (0,0)):\n    station_pixels = [[pixel[0], pixel[1]] for pixel in stations['pixel'].to_numpy()]\n    \n    # Get an array where each position represents pixel coordinates\n    tile_pixels = np.indices(shape).transpose(1,2,0).reshape(shape[0]*shape[1], 2) + offset\n    dist = distance_matrix(station_pixels, tile_pixels)\n    weights = np.where(dist == 0, np.finfo('float32').max, 1.0 / dist )\n    weights /=  weights.sum(axis=0)\n    \n    return np.dot(weights.T, stations[field_value].to_numpy()).reshape(shape).astype('float32')", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:58.209269Z", "start_time": "2021-04-13T14:38:58.202597Z"}, "trusted": true}, "execution_count": 25, "outputs": []}, {"cell_type": "markdown", "source": "Interpolate temperatures from a subset of the tile:", "metadata": {}}, {"cell_type": "code", "source": "def radiation_interpolation(obj, block_x, block_y, splits, storage):\n    tile_key = os.path.basename(obj.key)\n    tile_id, _ = os.path.splitext(tile_key)\n    \n    with rasterio.open(obj.data_stream) as src:\n        transform = src.transform\n        \n        # Compute working window\n        step_w = src.width / splits\n        step_h = src.height / splits\n        \n        offset_h = round(step_h * block_x)\n        offset_w = round(step_w * block_y)\n        \n        profile = src.profile\n        width = math.ceil(step_w * (block_y + 1) - offset_w)\n        height = math.ceil(step_h * (block_x + 1) - offset_h)\n        \n        profile.update(width=width)\n        profile.update(height=height)\n        \n        window = Window(offset_w, offset_h, width, height)\n        \n        with rasterio.open('input', 'w', **profile) as dest:\n            dest.write(src.read(window=window))\n        \n    # Stores global irradiation at \"output\", it also returns extraterrestrial irradiation\n    extraterrestrial_irradiation = compute_solar_irradiation('input', 'output')\n        \n    # Create and store a raster with extraterrestrial_irradiation\n    with rasterio.open('extr', 'w', **profile) as dest:\n        data = np.full((height, width), extraterrestrial_irradiation, dtype='float32')\n        dest.write(data, 1)\n        \n    out_key = os.path.join('tmp', 'extrad', tile_id, f'chunk_{block_x}-{block_y}') + '.tif'\n    with open('extr', 'rb') as out_file:\n        storage.put_object(BUCKET, out_key, out_file)\n    \n    out_key = os.path.join('tmp', 'rad', tile_id, f'chunk_{block_x}-{block_y}') + '.tif'\n    with open('output', 'rb') as out_file:\n        storage.put_object(BUCKET, out_key, out_file)\n    \n    return out_key", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:38:59.031150Z", "start_time": "2021-04-13T14:38:59.005158Z"}, "trusted": true}, "execution_count": 26, "outputs": []}, {"cell_type": "code", "source": "def map_interpolation(obj, block_x, block_y, splits, data_field, storage):\n    import io\n    import math\n    import numpy as np\n    \n    tile_key = os.path.basename(obj.key)\n    tile_id, _ = os.path.splitext(tile_key)\n          \n    siam_stream = storage.get_object(BUCKET, 'siam.csv', stream=True)\n    siam = pd.read_csv(siam_stream)\n    \n    with rasterio.open(obj.data_stream) as src:\n        transform = src.transform\n        \n        # Compute working window\n        step_w = src.width / splits\n        step_h = src.height / splits\n        \n        offset_h = round(step_h * block_x)\n        offset_w = round(step_w * block_y)\n        \n        profile = src.profile\n        \n        width = math.ceil(step_w * (block_y + 1) - offset_w)\n        height = math.ceil(step_h * (block_x + 1) - offset_h)\n        \n        profile.update(width=width)\n        profile.update(height=height)\n        \n        window = Window(offset_w,offset_h, width, height)\n        \n        # Filter desired stations\n        bounding_rect = box(src.bounds.left, src.bounds.top, src.bounds.right, src.bounds.bottom)\n        filtered = pd.DataFrame(filter_stations(bounding_rect, siam))\n        filtered['pixel'] = filtered.apply(\n            lambda station: rasterio.transform.rowcol(transform, station['X'], station['Y']), axis=1)\n        \n        # Interpolate and write results \n        with rasterio.open('output', 'w', **profile) as dest:\n            if data_field == 'temp':\n                elevations = src.read(1, window=window) # Get elevations content\n                interpolation = compute_basic_interpolation(elevations.shape, filtered,\n                                                            'tdet', (offset_h, offset_w))\n                interpolation += r * (elevations - zdet)\n                dest.write(np.where(elevations == src.nodata, np.nan, interpolation), 1)\n            else:\n                interpolation = compute_basic_interpolation((height, width), \n                                                            filtered, \n                                                            'hr' if data_field == 'humi' else 'v', \n                                                            (offset_h, offset_w))\n                dest.write(interpolation, 1)\n\n    # Export results to storage\n    out_key = os.path.join('tmp', data_field, tile_id, 'chunk_{}-{}'.format(block_x, block_y)) + '.tif'\n    with open('output', 'rb') as output_file:\n        storage.put_object(BUCKET, out_key, output_file)\n    \n    return out_key", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:39:01.538202Z", "start_time": "2021-04-13T14:39:01.521080Z"}, "trusted": true}, "execution_count": 27, "outputs": []}, {"cell_type": "markdown", "source": "Lithops serverless computation:", "metadata": {}}, {"cell_type": "code", "source": "lith = lithops.FunctionExecutor(backend='k8s', storage='ceph', runtime=RUNTIME)", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T16:58:43.408027Z", "start_time": "2021-04-13T16:58:42.454398Z"}, "trusted": true}, "execution_count": 28, "outputs": [{"name": "stderr", "text": "2021-08-31 11:11:21,517 [INFO] lithops.config -- Lithops v2.4.1.dev0\n2021-08-31 11:11:22,030 [INFO] lithops.storage.backends.ceph.ceph -- Ceph Storage client created - Endpoint: http://10.24.17.56:7480\n2021-08-31 11:11:22,070 [INFO] lithops.serverless.backends.k8s.k8s -- Kubernetes Job client created - Namespace: default\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "iterdata = [('ceph://{}/MDT/{}.tif'.format(BUCKET, tile), i, j) \n            for i in range(SPLITS) for j in range(SPLITS) for tile in tiles]", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:39:28.155159Z", "start_time": "2021-04-13T14:39:28.150797Z"}, "trusted": true}, "execution_count": 29, "outputs": []}, {"cell_type": "code", "source": "pprint(iterdata)\nprint('Total functions: {} tiles * ({}^2) splits * 4 calculations = {}'.format(\n    len(tiles), SPLITS, len(iterdata) * 4))", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:39:28.850404Z", "start_time": "2021-04-13T14:39:28.812552Z"}, "trusted": true}, "execution_count": 30, "outputs": [{"name": "stdout", "text": "[('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 0, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 0, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 0, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 0, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 0, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 0, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 0, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 0, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 0, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 0, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 0, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 0, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 0, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 0, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 0, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 0, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 0, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 0, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 0, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 0, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 0, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 0, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 0, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 0, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 1, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 1, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 1, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 1, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 1, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 1, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 1, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 1, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 1, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 1, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 1, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 1, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 1, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 1, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 1, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 1, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 1, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 1, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 1, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 1, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 1, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 1, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 1, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 1, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 2, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 2, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 2, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 2, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 2, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 2, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 2, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 2, 0),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 2, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 2, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 2, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 2, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 2, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 2, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 2, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 2, 1),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0891_LID.tif', 2, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0892_LID.tif', 2, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0912_LID.tif', 2, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0913_LID.tif', 2, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0933_LID.tif', 2, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0934_LID.tif', 2, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0954_LID.tif', 2, 2),\n ('ceph://geospatial-usecase/MDT/PNOA_MDT05_ETRS89_HU30_0955_LID.tif', 2, 2)]\nTotal functions: 8 tiles * (3^2) splits * 4 calculations = 288\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "fut = lith.map(radiation_interpolation, iterdata, extra_args=(SPLITS,), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T06:56:29.270356Z", "start_time": "2021-04-08T06:56:27.066344Z"}, "trusted": true}, "execution_count": null, "outputs": [{"name": "stderr", "text": "2021-08-31 11:11:22,322 [INFO] lithops.invokers -- ExecutorID 3daba0-0 | JobID M000 - Selected Runtime: tfmurv/lithops-k8s-geospatial - 2048MB\n2021-08-31 11:11:22,346 [INFO] lithops.invokers -- Runtime tfmurv/lithops-k8s-geospatial with 2048MB is not yet installed\n2021-08-31 11:11:22,347 [INFO] lithops.serverless.backends.k8s.k8s -- Extracting Python modules from: tfmurv/lithops-k8s-geospatial\n", "output_type": "stream"}]}, {"cell_type": "code", "source": "fut = lith.map(map_interpolation, iterdata, extra_args=(SPLITS,'temp'), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T06:56:30.638547Z", "start_time": "2021-04-08T06:56:29.277666Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "fut = lith.map(map_interpolation, iterdata, extra_args=(SPLITS,'humi'), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T06:56:32.429261Z", "start_time": "2021-04-08T06:56:31.673596Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "fut = lith.map(map_interpolation, iterdata, extra_args=(SPLITS,'wind'), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T06:56:34.748102Z", "start_time": "2021-04-08T06:56:34.186500Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "out_chunks = lith.get_result()", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:00:51.123143Z", "start_time": "2021-04-08T06:56:40.139182Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "---", "metadata": {}}, {"cell_type": "markdown", "source": "### KPIs (Interpolation)\n\n[Skip KPI section](#(End-of-KPI-section---Interpolation))", "metadata": {}}, {"cell_type": "code", "source": "lith.plot(dst=\"geospatial_usecase/plots/interpolation\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:02:05.040966Z", "start_time": "2021-04-08T07:02:04.321663Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "Image(filename=\"geospatial_usecase/plots/interpolation_histogram.png\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:02:45.854968Z", "start_time": "2021-04-08T07:02:45.848629Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "Image(filename=\"geospatial_usecase/plots/interpolation_timeline.png\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:02:54.046898Z", "start_time": "2021-04-08T07:02:54.036980Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### Data size ", "metadata": {}}, {"cell_type": "markdown", "source": "List of .tif files being processed:", "metadata": {}}, {"cell_type": "code", "source": "mdts_gtiff", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:03:29.258771Z", "start_time": "2021-04-08T07:03:29.255051Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Total size accounting that files were repeatedly processed:", "metadata": {}}, {"cell_type": "code", "source": "data_size = sum(obj[\"Size\"] for obj in cloud_storage.list_objects(BUCKET) if obj[\"Key\"] in mdts_gtiff)\ndata_size *= 4  # Each file was processed 4 times\n\nprint(f\"Data size: {data_size / 1024**2} MiB\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:03:33.021230Z", "start_time": "2021-04-08T07:03:32.728522Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### KPI: Cost", "metadata": {}}, {"cell_type": "code", "source": "lith.job_summary()", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:03:37.464531Z", "start_time": "2021-04-08T07:03:37.441706Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "pd.read_csv(lith.log_path)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:03:51.895064Z", "start_time": "2021-04-08T07:03:51.879123Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "cost_interpolation = get_process_cost(lith)\nprint(f\"The experiment cost ${cost_interpolation:.4f}.\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:03:39.286038Z", "start_time": "2021-04-08T07:03:39.276118Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### KPI: Throughput", "metadata": {}}, {"cell_type": "code", "source": "tstamps = set()\nfor future in lith.futures:\n    for key in future.stats.keys():\n        if key.endswith(\"tstamp\"):\n            tstamps.add(future.stats[key])\n            \nduration = max(tstamps) - min(tstamps)\nprint(\"Duration: \" + str(duration) + \" seconds\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:05:08.953755Z", "start_time": "2021-04-08T07:05:08.931643Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "throughput_interpolation = data_size / duration  # Bytes/second", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:05:11.960461Z", "start_time": "2021-04-08T07:05:11.953193Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "print(f\"Throughput: {throughput_interpolation / 1024**2} MiB/s\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:05:13.342171Z", "start_time": "2021-04-08T07:05:13.338300Z"}, "scrolled": true, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### KPI: Speedup", "metadata": {}}, {"cell_type": "markdown", "source": "In this section we compare the execution speed of a sample process performed in last section, using different amounts of parallel workers, in order to test the scalability of the process.", "metadata": {}}, {"cell_type": "code", "source": "parallel_workers = [12, 24, 48, 72]\nexperiment_duration = dict.fromkeys(parallel_workers)", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:39:50.231357Z", "start_time": "2021-04-13T14:39:50.225667Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Perform experiment several times and save duration:", "metadata": {}}, {"cell_type": "code", "source": "for option in parallel_workers:\n    lith = lithops.FunctionExecutor(\n        backend='k8s', \n        storage='ceph',\n        runtime=RUNTIME,\n        workers=option, # Tells lithops to work w/only this number of concurrent workers\n        log_level=\"DEBUG\"\n    )\n    fut = lith.map(\n        map_interpolation, iterdata, extra_args=(SPLITS,'temp'), runtime_memory=2048\n    )\n    lith.get_result()\n    \n    tstamps = set()\n    for future in lith.futures:\n        for key in future.stats.keys():\n            if key.endswith(\"tstamp\"):\n                tstamps.add(future.stats[key])\n    duration = max(tstamps) - min(tstamps)\n    experiment_duration[option] = duration", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:41:52.721071Z", "start_time": "2021-04-13T14:40:15.651918Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "experiment_duration", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T14:41:55.994144Z", "start_time": "2021-04-13T14:41:55.989057Z"}, "scrolled": true, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "##### Visualization of per-worker performance relative to first experiment", "metadata": {}}, {"cell_type": "markdown", "source": "The following plot represents two lines:\n- **Ideal speedup**: theoretical best speedup - scenario where a 2x increment in workers results in 1/2 execution time, a 4x increment in workers results in a 1/4 execution time, etc.\n- **Lithops speedup**: actual speedup that results from the experiment", "metadata": {}}, {"cell_type": "code", "source": "duration = list(experiment_duration.values())\ntheoretical_best_speedup = [(1 - parallel_workers[0] / parallel_workers[i]) * 100 for i in range(0, len(parallel_workers))]\nactual_speedup = [(1 - duration[i] / duration[0]) * 100 for i in range(0, len(duration))]\n\nplt.plot(\n    parallel_workers,\n    theoretical_best_speedup\n)\nplt.plot(\n    parallel_workers,\n    actual_speedup\n)\nplt.xlabel(\"Number of workers\")\nplt.ylabel(\"% time reduced, relative to first experiment\")\nplt.legend([\"Ideal speedup\", \"Lithops speedup (this experiment)\"])", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T15:09:52.888690Z", "start_time": "2021-04-13T15:09:52.723516Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Clean stats:", "metadata": {}}, {"cell_type": "code", "source": "lith.futures = []", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:11:22.584754Z", "start_time": "2021-04-08T07:11:22.577217Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "###### (End of KPI section - Interpolation)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:10:54.929461Z", "start_time": "2021-04-08T07:10:54.917364Z"}}}, {"cell_type": "markdown", "source": "---", "metadata": {}}, {"cell_type": "markdown", "source": "Join split subsets into a tile:", "metadata": {}}, {"cell_type": "code", "source": "def gather_blocks(tile, splits, data_field, storage):\n\n    from rasterio.windows import Window\n    \n    # Get width and height from original tile\n    with rasterio.open(storage.get_object(bucket=BUCKET, key=f'MDT/{tile}.tif', stream=True)) as og:\n        height = og.profile['height']\n        width = og.profile['width']\n    \n    chunk_tiles = storage.list_keys(bucket=BUCKET, prefix=f'tmp/{data_field}/{tile}/chunk')\n        \n    # Open first object to obtain profile metadata\n    with rasterio.open(storage.get_object(bucket=BUCKET, key=chunk_tiles[0], stream=True)) as src:\n        profile = src.profile\n        profile.update(width=width)\n        profile.update(height=height)\n\n    # Iterate each object and print its block into the destination file\n    with rasterio.open(\"output\", \"w\", **profile) as dest: \n        for chunk in chunk_tiles:\n            j, i = os.path.splitext(os.path.basename(chunk))[0].rsplit('_')[1].split('-')\n            j, i = int(j), int(i)\n            with rasterio.open(storage.get_object(bucket=BUCKET, key=chunk, stream=True)) as src:\n                step_w = math.floor(width / splits)\n                step_h = math.floor(height / splits)\n                curr_window = Window(round(step_w * i), round(step_h * j), src.width, src.height)\n                content = src.read(1)\n                dest.write(content, 1, window=curr_window)\n            # storage.delete_object(bucket=BUCKET, key=chunk)\n    \n    output_key = os.path.join('tmp', data_field, tile, '_'.join([tile, data_field.upper()+'.tif']))\n    with open('output', 'rb') as out_file:\n        storage.put_object(bucket=BUCKET, key=output_key, body=out_file)  \n    \n    return output_key", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:21:21.921233Z", "start_time": "2021-04-08T07:21:21.911497Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Combine previous split subsets:", "metadata": {}}, {"cell_type": "code", "source": "lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'extrad'), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:21:43.580625Z", "start_time": "2021-04-08T07:21:43.180508Z"}, "scrolled": true, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'humi'), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:21:44.784961Z", "start_time": "2021-04-08T07:21:44.401457Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'rad'), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:21:45.855437Z", "start_time": "2021-04-08T07:21:45.488359Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'temp'), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:21:46.912489Z", "start_time": "2021-04-08T07:21:46.515196Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "fut = lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'wind'), runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:21:48.282075Z", "start_time": "2021-04-08T07:21:47.908570Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "out_combined = lith.get_result()", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:22:14.948724Z", "start_time": "2021-04-08T07:21:52.356798Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### KPIs (Gather blocks)\n\n[Skip KPI section](#(End-of-KPI-section---Gather-blocks))", "metadata": {}}, {"cell_type": "code", "source": "lith.plot(dst=\"geospatial_usecase/plots/gather_blocks\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:23:42.594365Z", "start_time": "2021-04-08T07:23:42.135958Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "Image(filename=\"geospatial_usecase/plots/gather_blocks_histogram.png\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:23:44.168346Z", "start_time": "2021-04-08T07:23:44.162948Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "Image(filename=\"geospatial_usecase/plots/gather_blocks_timeline.png\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T07:23:45.856957Z", "start_time": "2021-04-08T07:23:45.840851Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### Data size", "metadata": {}}, {"cell_type": "code", "source": "mdts_gtiff", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:17:25.463278Z", "start_time": "2021-04-08T08:17:25.458630Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "data_size = sum(obj[\"Size\"] for obj in cloud_storage.list_objects(BUCKET) if obj[\"Key\"] in mdts_gtiff)\ndata_size *= 4  # Each file was processed 4 times\n\nprint(f\"Data size: {data_size / 1024**2} MiB\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:17:30.709533Z", "start_time": "2021-04-08T08:17:29.875330Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### KPI: Cost", "metadata": {}}, {"cell_type": "code", "source": "lith.job_summary()", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:18:54.513927Z", "start_time": "2021-04-08T08:18:54.486157Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "pd.read_csv(lith.log_path)", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:19:02.264749Z", "start_time": "2021-04-08T08:19:02.236457Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "cost_gather_blocks = get_process_cost(lith)\nprint(f\"The experiment cost ${cost_gather_blocks:.4f}.\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:21:09.051314Z", "start_time": "2021-04-08T08:21:09.031537Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### KPI: Throughput", "metadata": {}}, {"cell_type": "code", "source": "tstamps = set()\nfor future in lith.futures:\n    for key in future.stats.keys():\n        if key.endswith(\"tstamp\"):\n            tstamps.add(future.stats[key])\n            \nduration = max(tstamps) - min(tstamps)\nprint(\"Duration: \" + str(duration) + \" seconds\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:19:57.208334Z", "start_time": "2021-04-08T08:19:57.189780Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "throughput_gather_blocks = data_size / duration  # Bytes/second", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:21:20.407233Z", "start_time": "2021-04-08T08:21:20.404210Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "print(f\"Throughput: {throughput_gather_blocks / 1024**2} MiB/s\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:21:35.726076Z", "start_time": "2021-04-08T08:21:35.720504Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Clean stats:", "metadata": {}}, {"cell_type": "code", "source": "lith.futures = []", "metadata": {"ExecuteTime": {"end_time": "2021-04-08T08:22:57.849484Z", "start_time": "2021-04-08T08:22:57.844112Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "###### (End of KPI section - Gather blocks)", "metadata": {}}, {"cell_type": "markdown", "source": "---", "metadata": {}}, {"cell_type": "markdown", "source": "## Computation of potential evaporation", "metadata": {}}, {"cell_type": "code", "source": "def compute_crop_evapotranspiration(temperatures,\n                                    humidities,\n                                    wind_speeds,\n                                    external_radiations,\n                                    global_radiations,\n                                    KCs):\n    gamma = 0.665*101.3/1000\n    eSat = 0.6108 * np.exp((17.27*temperatures)/(temperatures+237.3))\n    delta = 4098 * eSat / np.power((temperatures + 237.3),2)\n    eA = np.where(humidities < 0, 0, eSat * humidities / 100)     # Avoid sqrt of a negative number\n    T4 = 4.903 * np.power((273.3 + temperatures),4)/1000000000\n    rSrS0 = global_radiations/(external_radiations * 0.75)\n    rN = 0.8* global_radiations-T4*(0.34-0.14*np.sqrt(eA))*((1.35*rSrS0)-0.35)\n    den = delta + gamma *(1 + 0.34* wind_speeds)\n    tRad = 0.408 * delta * rN / den\n    tAdv = gamma * (900/(temperatures+273))*wind_speeds * (eSat - eA)/den\n    return ((tRad + tAdv) * 7 * KCs).astype('float32')", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T16:57:51.493674Z", "start_time": "2021-04-13T16:57:51.485032Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "vineyard = ['VI', 'VO', 'VF', 'FV', 'CV' ]\nolive_grove = ['OV', 'VO', 'OF', 'FL', 'OC']\nfruit = ['FY', 'VF', 'OF', 'FF', 'CF']\nnuts = ['FS', 'FV', 'FL', 'FF', 'CS' ]\ncitrus = ['CI', 'CV', 'OC', 'CF', 'CS' ]\n\ndef get_kc(feature):\n    \n    # TODO: Get more precise values of Kc\n    sigpac_use = feature['properties']['uso_sigpac']\n    if sigpac_use in vineyard:\n        # Grapes for wine - 0.3, 0.7, 0.45\n        return 0.7  \n    if sigpac_use in olive_grove:\n        # Olive grove - ini: 0.65, med: 0.7, end: 0.7\n        return 0.7 \n    if sigpac_use in fruit:\n        # Apples, Cherries, Pears - 0.45, 0.95, 0.7\n        return 0.95\n    if sigpac_use in nuts:\n        # Almonds - 0.4, 0.9, 0.65\n        return 0.9\n    if sigpac_use in citrus:\n        # Citrus, without ground coverage - 0.7, 0.65, 0.7\n        return 0.65\n    \n    return None", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T16:57:52.760441Z", "start_time": "2021-04-13T16:57:52.753812Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "def get_geometry_window(src, geom_bounds):\n    left, bottom, right, top = geom_bounds\n    src_left, src_bottom, src_right, src_top = src.bounds\n    window = src.window(max(left,src_left), max(bottom,src_bottom), min(right,src_right), min(top,src_top))\n    window_floored = window.round_offsets(op='floor', pixel_precision=3)\n    w = math.ceil(window.width + window.col_off - window_floored.col_off)\n    h = math.ceil(window.height + window.row_off - window_floored.row_off)\n    return Window(window_floored.col_off, window_floored.row_off, w, h)     ", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T16:57:54.250115Z", "start_time": "2021-04-13T16:57:54.243932Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "def compute_evapotranspiration_by_shape(tem, hum, win, rad, extrad, dst):\n    \n    import fiona\n    from shapely.geometry import shape, box\n    from rasterio import features\n    \n    non_arable_land = ['AG', 'CA', 'ED', 'FO', 'IM', 'PA', 'PR', 'ZU', 'ZV']\n    \n    with fiona.open('zip://shape.zip') as shape_src:\n        for feature in shape_src.filter(bbox=tem.bounds):\n            KC = get_kc(feature) \n            if KC is not None:   \n                geom = shape(feature['geometry'])  \n                window = get_geometry_window(tem, geom.bounds)              \n                win_transform = rasterio.windows.transform(window, tem.transform)\n                # Convert shape to raster matrix\n                image = features.rasterize([geom],\n                                           out_shape=(window.height, window.width),\n                                           transform = win_transform,\n                                           fill = 0,\n                                           default_value = 1).astype('bool')\n                # Get values to compute evapotranspiration\n                temperatures = tem.read(1, window=window)\n                humidities = hum.read(1, window=window)\n                wind_speeds = win.read(1, window=window)\n                # Convert from W to MJ (0.0036)\n                global_radiations = rad.read(1, window=window) * 0.0036\n                external_radiations = extrad.read(1, window=window) * 0.0036\n                KCs = np.full(temperatures.shape, KC)\n                # TODO: compute external radiation\n                #external_radiations = np.full(temperatures.shape, 14)\n                # TODO: compute global radiation\n                # global_radiations = np.full(temperatures.shape, 10)\n                etc = compute_crop_evapotranspiration(\n                        temperatures,\n                        humidities,\n                        wind_speeds,\n                        external_radiations,\n                        global_radiations,\n                        KCs\n                )\n                etc[temperatures == tem.nodata] = dst.nodata\n                etc[np.logical_not(image)] = dst.nodata\n                dst.write(etc + dst.read(1, window=window), 1, window=window)", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T16:57:57.781920Z", "start_time": "2021-04-13T16:57:57.770029Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "def compute_global_evapotranspiration(tem, hum, win, rad, extrad, dst):    \n    for ji, window in tem.block_windows(1):\n        bounds = rasterio.windows.bounds(window, tem.transform)\n        temperatures = tem.read(1, window=window)\n        humidities = hum.read(1, window=window)\n        wind_speeds = win.read(1, window=window)\n         # Convert from W to MJ (0.0036)\n        global_radiations = rad.read(1, window=window) * 0.0036\n        external_radiations = extrad.read(1, window=window) * 0.0036\n        # TODO: compute external radiation\n        #external_radiations = np.full(temperatures.shape, 14)\n        # TODO: compute global radiation\n        # global_radiations = np.full(temperatures.shape, 10)\n        # TODO: compute KCs\n        KCs = np.full(temperatures.shape, 1)\n        etc = compute_crop_evapotranspiration(\n                temperatures,\n                humidities,\n                wind_speeds,\n                external_radiations,\n                global_radiations,\n                KCs\n        )\n        dst.write(np.where(temperatures == tem.nodata, dst.nodata, etc), 1, window=window)", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T16:57:59.216824Z", "start_time": "2021-04-13T16:57:59.207435Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "def combine_calculations(tile, storage):\n    \n    from functools import partial\n      \n    # Download shapefile\n    shapefile = storage.get_object(bucket=BUCKET, key='shapefile.zip', stream=True)\n    with open('shape.zip', 'wb') as shapf:\n        for chunk in iter(partial(shapefile.read, 200 * 1024 * 1024), ''):\n            if not chunk:\n                break\n            shapf.write(chunk)\n    \n    temp = storage.get_object(bucket=BUCKET, key=f'tmp/temp/{tile}/{tile}_TEMP.tif', stream=True)\n    humi = storage.get_object(bucket=BUCKET, key=f'tmp/humi/{tile}/{tile}_HUMI.tif', stream=True)\n    rad = storage.get_object(bucket=BUCKET, key=f'tmp/rad/{tile}/{tile}_RAD.tif', stream=True)\n    extrad = storage.get_object(bucket=BUCKET, key=f'tmp/extrad/{tile}/{tile}_EXTRAD.tif', stream=True)\n    wind = storage.get_object(bucket=BUCKET, key=f'tmp/wind/{tile}/{tile}_WIND.tif', stream=True)\n    \n    with rasterio.open(temp) as temp_raster:\n        with rasterio.open(humi) as humi_raster:\n            with rasterio.open(rad) as rad_raster:\n                with rasterio.open(extrad) as extrad_raster:\n                    with rasterio.open(wind) as wind_raster:\n                        profile = temp_raster.profile\n                        profile.update(nodata=0)\n        \n                        with rasterio.open('output', 'w+', **profile) as dst:\n#                             compute_global_evapotranspiration(temp_raster, humi_raster, wind_raster,\n#                                                               rad_raster, extrad_raster, dst)\n                            compute_evapotranspiration_by_shape(temp_raster, humi_raster, wind_raster,\n                                                                rad_raster, extrad_raster, dst)\n    \n    out_key = f'etc/{tile}_ETC.tif'\n    with open('output', 'rb') as output_f:\n        storage.put_object(bucket=BUCKET, key=out_key, body=output_f)\n    return out_key", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T16:58:01.128416Z", "start_time": "2021-04-13T16:58:01.101842Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "fut = lith.map(combine_calculations, tiles, runtime_memory=2048)", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T16:58:55.822484Z", "start_time": "2021-04-13T16:58:54.943303Z"}, "scrolled": true, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "res = lith.get_result()", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T17:01:57.663322Z", "start_time": "2021-04-13T16:58:57.632356Z"}, "scrolled": true, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### KPIs (Potential evaporation)\n\n[Skip KPI section](#(End-of-KPI-section---Potential-evaporation))", "metadata": {}}, {"cell_type": "code", "source": "lith.plot(dst=\"geospatial_usecase/plots/potential_evaporation\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:06:46.567960Z", "start_time": "2021-04-13T20:06:45.870566Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "Image(filename=\"geospatial_usecase/plots/potential_evaporation_histogram.png\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:07:04.735119Z", "start_time": "2021-04-13T20:07:04.729026Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "Image(filename=\"geospatial_usecase/plots/potential_evaporation_timeline.png\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:07:08.400698Z", "start_time": "2021-04-13T20:07:08.395328Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### Data size", "metadata": {}}, {"cell_type": "code", "source": "tiles", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:08:12.027411Z", "start_time": "2021-04-13T20:08:12.018635Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "data_size = 0\n\nfor obj in cloud_storage.list_objects(BUCKET):\n    for tile in tiles:\n        if obj[\"Key\"] == f'tmp/temp/{tile}/{tile}_TEMP.tif' or \\\n                obj[\"Key\"] == f'tmp/humi/{tile}/{tile}_TEMP.tif' or \\\n                obj[\"Key\"] == f'tmp/rad/{tile}/{tile}_TEMP.tif' or \\\n                obj[\"Key\"] == f'tmp/extrad/{tile}/{tile}_TEMP.tif' or \\\n                obj[\"Key\"] == f'tmp/wind/{tile}/{tile}_TEMP.tif' or \\\n                obj[\"Key\"] == 'shapefile.zip':\n            data_size += obj[\"Size\"]\n\nprint(f\"Data size: {data_size / 1024**2} MiB\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:54:33.848670Z", "start_time": "2021-04-13T20:54:33.645465Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### KPI: Cost", "metadata": {}}, {"cell_type": "code", "source": "lith.job_summary()", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:54:45.609853Z", "start_time": "2021-04-13T20:54:45.592066Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "pd.read_csv(lith.log_path)", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:07:16.935146Z", "start_time": "2021-04-13T20:07:16.918524Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "cost_potential_evaporation = get_process_cost(lith)\nprint(f\"The experiment cost ${cost_potential_evaporation:.4f}.\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:07:26.235760Z", "start_time": "2021-04-13T20:07:26.226071Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### KPI: Throughput", "metadata": {}}, {"cell_type": "code", "source": "tstamps = set()\nfor future in lith.futures:\n    for key in future.stats.keys():\n        if key.endswith(\"tstamp\"):\n            tstamps.add(future.stats[key])\n            \nduration = max(tstamps) - min(tstamps)\nprint(\"Duration: \" + str(duration) + \" seconds\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:55:04.595689Z", "start_time": "2021-04-13T20:55:04.581005Z"}, "tags": [], "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "throughput_potential_evaporation = data_size / duration  # Bytes/second", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:55:07.194755Z", "start_time": "2021-04-13T20:55:07.192029Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "print(f\"Throughput: {throughput_potential_evaporation / 1024**2} MiB/s\")", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T20:55:09.220719Z", "start_time": "2021-04-13T20:55:09.212358Z"}, "scrolled": true, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "#### KPI: Speedup", "metadata": {}}, {"cell_type": "markdown", "source": "In this section we compare the execution speed of a sample process performed in last section, using different amounts of parallel workers, in order to test the scalability of the process.", "metadata": {}}, {"cell_type": "code", "source": "parallel_workers = [2, 4, 8]\nexperiment_duration = dict.fromkeys(parallel_workers)", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:10:15.560833Z", "start_time": "2021-04-13T21:10:15.556295Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Perform experiment several times and save duration:", "metadata": {}}, {"cell_type": "code", "source": "for option in parallel_workers:\n    lith = lithops.FunctionExecutor(\n        backend='k8s', \n        storage='ceph', \n        runtime=RUNTIME,\n        workers=option, # Tells lithops to work w/only this number of concurrent workers\n        log_level=\"NOTSET\"\n    )\n    lith.map(combine_calculations, tiles, runtime_memory=2048)\n    lith.get_result()\n    \n    tstamps = set()\n    for future in lith.futures:\n        for key in future.stats.keys():\n            if key.endswith(\"tstamp\"):\n                tstamps.add(future.stats[key])\n    duration = max(tstamps) - min(tstamps)\n    experiment_duration[option] = duration", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:26:47.958535Z", "start_time": "2021-04-13T21:10:17.409017Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "experiment_duration", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:29:20.900597Z", "start_time": "2021-04-13T21:29:20.893752Z"}, "scrolled": true, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "##### Visualization of per-worker performance relative to first experiment", "metadata": {}}, {"cell_type": "markdown", "source": "The following plot represents two lines:\n- **Ideal speedup**: theoretical best speedup - scenario where a 2x increment in workers results in 1/2 execution time, a 4x increment in workers results in a 1/4 execution time, etc.\n- **Lithops speedup**: actual speedup that results from the experiment", "metadata": {}}, {"cell_type": "code", "source": "duration = list(experiment_duration.values())\ntheoretical_best_speedup = [(1 - parallel_workers[0] / parallel_workers[i]) * 100 for i in range(0, len(parallel_workers))]\nactual_speedup = [(1 - duration[i] / duration[0]) * 100 for i in range(0, len(duration))]\n\nplt.plot(\n    parallel_workers,\n    theoretical_best_speedup\n)\nplt.plot(\n    parallel_workers,\n    actual_speedup\n)\nplt.xlabel(\"Number of workers\")\nplt.ylabel(\"% time reduced, relative to first experiment\")\nplt.legend([\"Ideal speedup\", \"Lithops speedup (this experiment)\"])", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:29:33.018923Z", "start_time": "2021-04-13T21:29:32.780739Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Clean stats:", "metadata": {}}, {"cell_type": "code", "source": "lith.futures = []", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "###### (End of KPI section - Potential evaporation)", "metadata": {}}, {"cell_type": "markdown", "source": "---", "metadata": {}}, {"cell_type": "markdown", "source": "## Visualization of results", "metadata": {}}, {"cell_type": "code", "source": "import io\ntile = random.choice(tiles)\nobj = io.BytesIO(cloud_storage.get_object(bucket=BUCKET, key=f'etc/{tile}_ETC.tif'))", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:30:27.539704Z", "start_time": "2021-04-13T21:30:26.276005Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "from matplotlib import pyplot as plt\n\nfig, ax = plt.subplots()\n\nwith rasterio.open(obj) as src:\n    arr = src.read(1, out_shape=(src.height, src.width))\n    ax.set_title(tile)\n    img = ax.imshow(arr, cmap='Greens')\n    fig.colorbar(img, shrink=0.5)\n\nfig.set_size_inches(18.5, 10.5)\nplt.show()\n\nobj.seek(0)", "metadata": {"ExecuteTime": {"end_time": "2021-04-13T21:32:01.429761Z", "start_time": "2021-04-13T21:32:00.245568Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### Utility functions", "metadata": {}}, {"cell_type": "markdown", "source": "Remove intermediate data", "metadata": {}}, {"cell_type": "code", "source": "# keys = cloud_storage.list_keys(bucket=BUCKET, prefix='')\n# keys", "metadata": {"ExecuteTime": {"end_time": "2021-03-29T10:06:07.571233Z", "start_time": "2021-03-29T10:05:14.526Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "# for key in keys:\n#     cloud_storage.delete_object(bucket=BUCKET, key=key)", "metadata": {"ExecuteTime": {"end_time": "2021-03-29T10:06:07.574068Z", "start_time": "2021-03-29T10:05:14.528Z"}, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}]}