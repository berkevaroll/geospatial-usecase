{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:25:24.015057Z",
     "start_time": "2021-04-07T09:25:24.010199Z"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version:** Performance analysis\n",
    "\n",
    "*(Sergi Domingo sergi.domingo@urv.cat)*\n",
    "\n",
    "Check available KPIs at:\n",
    "\n",
    "- [Interpolation](#KPIs-(Interpolation))\n",
    "- [Gather blocks](#KPIs-(Gather-blocks))\n",
    "- [Potential evaporation](#KPIs-(Potential-evaporation))\n",
    "\n",
    "**Summary of a sample execution**:\n",
    "\n",
    "- Interpolation\n",
    "    - The whole interpolation process (map interpolation + radiation interpolation) was distributed across 288 Cloud Functions (each allocated 2048MB).\n",
    "    - The computational process for ~2GiB of .TIFF map data lasted around ~4 minutes. \n",
    "    - Throughput was 7.2295 MiB/s.\n",
    "    - The process cost ~0.30 dollars.\n",
    "    - Speedup for a part of the process was computed and the results showed excellent scalability (90% of the ideal speedup) for experiments under 100 parallel Cloud Functions, compared to a base case.\n",
    "    \n",
    "    \n",
    "- Gather blocks\n",
    "    - This quick process deployed 40 Cloud Functions (each allocated 2048MB) to join the ~2GiB of independent chunk data into bigger blocks.\n",
    "    - Throughput was 60.1322 MiB/s.\n",
    "    - The process lasted for ~30s and cost ~0.02 dollars.\n",
    "   \n",
    "   \n",
    "- Potential evaporation\n",
    "    - Lithops divided the potential evaporation process into 8 Cloud Functions.\n",
    "    - The process allocated 16GiB of memory and computed ~6GiB of .TIFF files in ~3 minutes.\n",
    "    - The cost of the process was ~0.03 dollars.\n",
    "    - Throughput was 33.9837 MiB/s.\n",
    "    - Speedup compared to a base case of 2 workers was above 92% for experiments run with 4 and 8 workers.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:23.745397Z",
     "start_time": "2021-04-13T14:38:23.312419Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:24.181256Z",
     "start_time": "2021-04-13T14:38:24.175102Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_process_cost(lith):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(lith.log_path)\n",
    "    cost = float(df[df[\"Job_ID\"] == \"Summary\"][\"Cost\"])\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current notebook computes an interpolation of temperatures in each pixel based on SIAM extracted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A .bluemix/cos_credentials.json file correctly configured located at home directory is needed in order to connect with IBM Cloud. More information at https://cloud.ibm.com/docs/services/cloud-object-storage/iam?topic=cloud-object-storage-service-credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:27.706148Z",
     "start_time": "2021-04-13T14:38:27.027515Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from ibm_botocore.client import Config, ClientError\n",
    "from geospatial_usecase.io_utils.cos import COS\n",
    "from geospatial_usecase.io_utils.plot import plot_random_blocks, plot_results\n",
    "from rasterio.windows import Window\n",
    "from scipy.spatial import distance_matrix\n",
    "from shapely.geometry import Point, MultiPoint, box\n",
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "import ibm_boto3\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lithops\n",
    "import requests\n",
    "import rasterio\n",
    "import json\n",
    "import random\n",
    "\n",
    "from lithops.storage import Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Area outside the processed tile that we want to consider for taking SIAM stations into account:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:28.795793Z",
     "start_time": "2021-04-13T14:38:28.788173Z"
    }
   },
   "outputs": [],
   "source": [
    "AREA_OF_INFLUENCE = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IBM Cos bucket to upload files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:29.658886Z",
     "start_time": "2021-04-13T14:38:29.654251Z"
    }
   },
   "outputs": [],
   "source": [
    "BUCKET = 'geospatial-usecase'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNTIME = 'sergidomingo/geospatial-k8s:jt1'\n",
    "# RUNTIME = 'jsampe/lithops-k8s-geospatial'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split tile into SPLITS$^2$ chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:30.525082Z",
     "start_time": "2021-04-13T14:38:30.519883Z"
    }
   },
   "outputs": [],
   "source": [
    "SPLITS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation coefficient between elevation and temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:31.362634Z",
     "start_time": "2021-04-13T14:38:31.359578Z"
    }
   },
   "outputs": [],
   "source": [
    "r = -0.0056"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elevation to interpolate temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:32.187402Z",
     "start_time": "2021-04-13T14:38:32.184798Z"
    }
   },
   "outputs": [],
   "source": [
    "zdet = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Day of year to calculate solar irradiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:33.304420Z",
     "start_time": "2021-04-13T14:38:33.299098Z"
    }
   },
   "outputs": [],
   "source": [
    "DAY_OF_YEAR = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object storage key prefix, to keep objects organized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:34.978991Z",
     "start_time": "2021-04-13T14:38:34.905888Z"
    }
   },
   "outputs": [],
   "source": [
    "cloud_storage = Storage(backend=\"ceph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section fetches and uploads to COS the metadata used in the workflow. It can be skipped if the data is already in COS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIAM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:42:03.976592Z",
     "start_time": "2021-04-07T09:42:02.944388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'http://siam.imida.es/apex/f?p=101:47:493289053024037:CSV::::'\n",
    "# url = 'http://siam.imida.es/apex/f?p=101:48:2555846978143339:CSV::::'\n",
    "siam_data = requests.get(url)\n",
    "with open('geospatial_usecase/siam_data.csv', 'wb') as siam_data_file:\n",
    "    siam_data_file.write(siam_data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:42:06.892586Z",
     "start_time": "2021-04-07T09:42:06.886566Z"
    }
   },
   "outputs": [],
   "source": [
    "def guess_nearest(x, y, field, stations):\n",
    "    '''\n",
    "    Compute field value at a given x,y point by getting the value of the closest station \n",
    "    '''\n",
    "    from shapely.ops import nearest_points\n",
    "    stations_of_interest = stations[(stations[field] != '-') & ((stations['X'] != x) | (stations['Y'] != y))]\n",
    "    points = MultiPoint(stations_of_interest.apply(lambda row: Point(row['X'], row['Y']), axis=1 ).array)\n",
    "    nearest = nearest_points(Point(x,y), points)[1]\n",
    "    val = stations_of_interest[(stations_of_interest['X'] == nearest.x) &\n",
    "                                (stations_of_interest['Y'] == nearest.y)]\n",
    "\n",
    "    return stations_of_interest[(stations_of_interest['X'] == nearest.x) &\n",
    "                                (stations_of_interest['Y'] == nearest.y)][field].iloc[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append locations of SIAM stations to previously downloaded data and write results to a CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:42:12.829971Z",
     "start_time": "2021-04-07T09:42:12.755771Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = {\n",
    "    'Estación': 'COD',\n",
    "    'Tmed <br> (ºC)': 'temp',\n",
    "    'Hrmed <br> (%)': 'hr',\n",
    "    'Vvmed <br> (m/seg)': 'v',\n",
    "    'Eti.': 'dir',\n",
    "    'Radmed <br> (w/m2)': 'rad',\n",
    "    'Dvmed <br>  (º)': 'dir_deg'\n",
    "}\n",
    "\n",
    "siam_data = pd.read_csv('geospatial_usecase/siam_data.csv', encoding='iso-8859-1',\n",
    "                        sep=';', decimal=',', thousands='.', na_values = '-')\n",
    "siam_data = siam_data[columns.keys()].rename(columns=columns)\n",
    "siam_locations = pd.read_csv('geospatial_usecase/siam_locations.csv', encoding='iso-8859-1', sep=';', decimal = ',', thousands='.')\n",
    "siam = pd.merge(siam_locations, siam_data, on='COD')\n",
    "siam['tdet'] = siam['temp'] + r * (zdet - siam['Cota'].to_numpy())\n",
    "siam = siam[['X', 'Y', 'Cota', 'temp', 'hr', 'tdet', 'v'] + list(columns.values())]\n",
    "# Guess wind direction of undefined values\n",
    "siam['dir_deg'] = siam.apply(lambda row: row['dir_deg'] \n",
    "                                     if not math.isnan(row['dir_deg'])\n",
    "                                     else guess_nearest(row['X'], row['Y'], 'dir_deg', siam), axis=1)\n",
    "# Guess radiation of undefined values\n",
    "siam['rad'] = siam.apply(lambda row: row['rad'] \n",
    "                                     if not math.isnan(row['rad'])\n",
    "                                     else guess_nearest(row['X'], row['Y'], 'rad', siam), axis=1)\n",
    "siam.to_csv('geospatial_usecase/siam.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the output CSV to COS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:46:49.948481Z",
     "start_time": "2021-04-07T09:46:49.800147Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('geospatial_usecase/siam.csv', 'rb') as siam_out_file:\n",
    "    cloud_storage.put_object(bucket=BUCKET, key='siam.csv', body=siam_out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MDT (Modelo digital del terreno) data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download MDT files for free from http://centrodedescargas.cnig.es/CentroDescargas/buscadorCatalogo.do?codFamilia=MDT05# and put them in `MDT` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find downloaded MDTs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:48:12.000399Z",
     "start_time": "2021-04-07T09:48:11.986192Z"
    }
   },
   "outputs": [],
   "source": [
    "mdt_folder = 'geospatial_usecase/MDT'\n",
    "mdts = [os.path.join(mdt_folder, mdt) for mdt in os.listdir(mdt_folder) if mdt.endswith('.asc')]\n",
    "mdts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:48:13.697506Z",
     "start_time": "2021-04-07T09:48:13.691877Z"
    }
   },
   "outputs": [],
   "source": [
    "tiles = [os.path.splitext(os.path.basename(mdt))[0] for mdt in mdts]\n",
    "tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert digital elevation map into a Cloud Optimized Geotiff. Upload then to IBM COS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:49:39.303588Z",
     "start_time": "2021-04-07T09:48:35.778105Z"
    }
   },
   "outputs": [],
   "source": [
    "for mdt in mdts:\n",
    "    tiff_file = os.path.splitext(mdt)[0] + '.tif'\n",
    "    with rasterio.open(mdt) as src:\n",
    "        profile = src.profile\n",
    "        # Cloud optimized GeoTiff parameters (No hace falta rio_cogeo)\n",
    "        profile.update(driver='GTiff')\n",
    "        profile.update(blockxsize=256)\n",
    "        profile.update(blockysize=256)\n",
    "        profile.update(tiled=True)\n",
    "        profile.update(compress='deflate')\n",
    "        profile.update(interleave='band')\n",
    "        with rasterio.open(tiff_file, \"w\", **profile) as dest:\n",
    "            dest.write(src.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:47.839384Z",
     "start_time": "2021-04-13T14:38:47.833205Z"
    }
   },
   "outputs": [],
   "source": [
    "mdts_gtiff = [os.path.join(mdt_folder, mdt) for mdt in os.listdir(mdt_folder) if mdt.endswith('.tif')]\n",
    "mdts_gtiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-07T09:51:19.779414Z",
     "start_time": "2021-04-07T09:50:38.654162Z"
    }
   },
   "outputs": [],
   "source": [
    "for mdt_gtiff in mdts_gtiff:\n",
    "    with open(mdt_gtiff, 'rb') as mdt_file:\n",
    "        cloud_storage.put_object(bucket=BUCKET, key=mdt_gtiff, body=mdt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serverless computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input MDT tiles to process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:50.635461Z",
     "start_time": "2021-04-13T14:38:50.327866Z"
    }
   },
   "outputs": [],
   "source": [
    "tiles = [os.path.splitext(os.path.basename(key))[0]\n",
    "         for key in cloud_storage.list_keys(bucket=BUCKET, prefix='MDT')]\n",
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T10:06:07.523375Z",
     "start_time": "2021-03-29T10:05:14.468Z"
    }
   },
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# tiles = ['PNOA_MDT05_ETRS89_HU30_0933_LID']\n",
    "# tiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute solar irradiation given a day of year using GRASS libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:54.462039Z",
     "start_time": "2021-04-13T14:38:54.449706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_solar_irradiation(inputFile, outputFile, crs='32630'):\n",
    "    # Define grass working set\n",
    "    GRASS_GISDB = 'grassdata'\n",
    "    GRASS_LOCATION = 'GEOPROCESSING'\n",
    "    GRASS_MAPSET = 'PERMANENT'\n",
    "    GRASS_ELEVATIONS_FILENAME = 'ELEVATIONS'\n",
    "    \n",
    "    import os\n",
    "    import shutil\n",
    "    os.environ['GRASSBIN'] = 'grass76'\n",
    "    from grass_session import Session\n",
    "    import grass.script as gscript\n",
    "    from grass.pygrass.modules.shortcuts import general as g\n",
    "    from grass.pygrass.modules.shortcuts import raster as r\n",
    "    import re\n",
    "    os.environ.update(dict(GRASS_COMPRESS_NULLS='1'))\n",
    "    \n",
    "    # Clean previously processed data\n",
    "    if os.path.isdir(GRASS_GISDB):\n",
    "        shutil.rmtree(GRASS_GISDB)\n",
    "    with Session(gisdb=GRASS_GISDB, location=GRASS_LOCATION, mapset=GRASS_MAPSET, create_opts='EPSG:32630') as ses:\n",
    "    \n",
    "        # Set project projection to match elevation raster projection\n",
    "        g.proj(epsg=crs, flags='c') \n",
    "    \n",
    "        # Load raster file into working directory\n",
    "        r.import_(input=inputFile, \n",
    "                  output=GRASS_ELEVATIONS_FILENAME, \n",
    "                  flags='o')    \n",
    "        \n",
    "        # Set project region to match raster region\n",
    "        g.region(raster=GRASS_ELEVATIONS_FILENAME, flags='s')    \n",
    "        # Calculate solar irradiation\n",
    "        gscript.run_command('r.slope.aspect', elevation=GRASS_ELEVATIONS_FILENAME,\n",
    "                            slope='slope', aspect='aspect')\n",
    "        gscript.run_command('r.sun', elevation=GRASS_ELEVATIONS_FILENAME,\n",
    "                            slope='slope', aspect='aspect', beam_rad='beam',\n",
    "                            step=1, day=DAY_OF_YEAR)\n",
    "        \n",
    "        # Get extraterrestrial irradiation from history metadata\n",
    "        regex = re.compile(r'\\d+\\.\\d+')\n",
    "        output = gscript.read_command(\"r.info\", flags=\"h\", map=[\"beam\"])\n",
    "        splits = str(output).split('\\n')\n",
    "        line = next(filter(lambda line: 'Extraterrestrial' in line, splits))\n",
    "        extraterrestrial_irradiance = float(regex.search(line)[0])\n",
    "        \n",
    "        # Export generated results into a GeoTiff file\n",
    "        if os.path.isfile(outputFile):\n",
    "            os.remove(outputFile)\n",
    "            \n",
    "        r.out_gdal(input='beam', output=outputFile)\n",
    "        \n",
    "        return extraterrestrial_irradiance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get stations contained in the area of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:55.461635Z",
     "start_time": "2021-04-13T14:38:55.457230Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_stations(bounds, stations):\n",
    "    total_points = MultiPoint([Point(x,y) for x, y in stations[['X', 'Y']].to_numpy()])\n",
    "    intersection = bounds.buffer(AREA_OF_INFLUENCE).intersection(total_points)\n",
    "    \n",
    "    return stations[[ intersection.contains(point) for point in total_points]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse Distance Weighting interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:58.209269Z",
     "start_time": "2021-04-13T14:38:58.202597Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_basic_interpolation(shape, stations, field_value, offset = (0,0)):\n",
    "    station_pixels = [[pixel[0], pixel[1]] for pixel in stations['pixel'].to_numpy()]\n",
    "    \n",
    "    # Get an array where each position represents pixel coordinates\n",
    "    tile_pixels = np.indices(shape).transpose(1,2,0).reshape(shape[0]*shape[1], 2) + offset\n",
    "    dist = distance_matrix(station_pixels, tile_pixels)\n",
    "    weights = np.where(dist == 0, np.finfo('float32').max, 1.0 / dist )\n",
    "    weights /=  weights.sum(axis=0)\n",
    "    \n",
    "    return np.dot(weights.T, stations[field_value].to_numpy()).reshape(shape).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate temperatures from a subset of the tile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:38:59.031150Z",
     "start_time": "2021-04-13T14:38:59.005158Z"
    }
   },
   "outputs": [],
   "source": [
    "def radiation_interpolation(obj, block_x, block_y, splits, storage):\n",
    "    tile_key = os.path.basename(obj.key)\n",
    "    tile_id, _ = os.path.splitext(tile_key)\n",
    "    \n",
    "    with rasterio.open(obj.data_stream) as src:\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Compute working window\n",
    "        step_w = src.width / splits\n",
    "        step_h = src.height / splits\n",
    "        \n",
    "        offset_h = round(step_h * block_x)\n",
    "        offset_w = round(step_w * block_y)\n",
    "        \n",
    "        profile = src.profile\n",
    "        width = math.ceil(step_w * (block_y + 1) - offset_w)\n",
    "        height = math.ceil(step_h * (block_x + 1) - offset_h)\n",
    "        \n",
    "        profile.update(width=width)\n",
    "        profile.update(height=height)\n",
    "        \n",
    "        window = Window(offset_w, offset_h, width, height)\n",
    "        \n",
    "        with rasterio.open('input', 'w', **profile) as dest:\n",
    "            dest.write(src.read(window=window))\n",
    "        \n",
    "    # Stores global irradiation at \"output\", it also returns extraterrestrial irradiation\n",
    "    extraterrestrial_irradiation = compute_solar_irradiation('input', 'output')\n",
    "        \n",
    "    # Create and store a raster with extraterrestrial_irradiation\n",
    "    with rasterio.open('extr', 'w', **profile) as dest:\n",
    "        data = np.full((height, width), extraterrestrial_irradiation, dtype='float32')\n",
    "        dest.write(data, 1)\n",
    "        \n",
    "    out_key = os.path.join('tmp', 'extrad', tile_id, f'chunk_{block_x}-{block_y}') + '.tif'\n",
    "    with open('extr', 'rb') as out_file:\n",
    "        storage.put_object(BUCKET, out_key, out_file)\n",
    "    \n",
    "    out_key = os.path.join('tmp', 'rad', tile_id, f'chunk_{block_x}-{block_y}') + '.tif'\n",
    "    with open('output', 'rb') as out_file:\n",
    "        storage.put_object(BUCKET, out_key, out_file)\n",
    "    \n",
    "    return out_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:39:01.538202Z",
     "start_time": "2021-04-13T14:39:01.521080Z"
    }
   },
   "outputs": [],
   "source": [
    "def map_interpolation(obj, block_x, block_y, splits, data_field, storage):\n",
    "    import io\n",
    "    import math\n",
    "    import numpy as np\n",
    "    \n",
    "    tile_key = os.path.basename(obj.key)\n",
    "    tile_id, _ = os.path.splitext(tile_key)\n",
    "          \n",
    "    siam_stream = storage.get_object(BUCKET, 'siam.csv', stream=True)\n",
    "    siam = pd.read_csv(siam_stream)\n",
    "    \n",
    "    with rasterio.open(obj.data_stream) as src:\n",
    "        transform = src.transform\n",
    "        \n",
    "        # Compute working window\n",
    "        step_w = src.width / splits\n",
    "        step_h = src.height / splits\n",
    "        \n",
    "        offset_h = round(step_h * block_x)\n",
    "        offset_w = round(step_w * block_y)\n",
    "        \n",
    "        profile = src.profile\n",
    "        \n",
    "        width = math.ceil(step_w * (block_y + 1) - offset_w)\n",
    "        height = math.ceil(step_h * (block_x + 1) - offset_h)\n",
    "        \n",
    "        profile.update(width=width)\n",
    "        profile.update(height=height)\n",
    "        \n",
    "        window = Window(offset_w,offset_h, width, height)\n",
    "        \n",
    "        # Filter desired stations\n",
    "        bounding_rect = box(src.bounds.left, src.bounds.top, src.bounds.right, src.bounds.bottom)\n",
    "        filtered = pd.DataFrame(filter_stations(bounding_rect, siam))\n",
    "        filtered['pixel'] = filtered.apply(\n",
    "            lambda station: rasterio.transform.rowcol(transform, station['X'], station['Y']), axis=1)\n",
    "        \n",
    "        # Interpolate and write results \n",
    "        with rasterio.open('output', 'w', **profile) as dest:\n",
    "            if data_field == 'temp':\n",
    "                elevations = src.read(1, window=window) # Get elevations content\n",
    "                interpolation = compute_basic_interpolation(elevations.shape, filtered,\n",
    "                                                            'tdet', (offset_h, offset_w))\n",
    "                interpolation += r * (elevations - zdet)\n",
    "                dest.write(np.where(elevations == src.nodata, np.nan, interpolation), 1)\n",
    "            else:\n",
    "                interpolation = compute_basic_interpolation((height, width), \n",
    "                                                            filtered, \n",
    "                                                            'hr' if data_field == 'humi' else 'v', \n",
    "                                                            (offset_h, offset_w))\n",
    "                dest.write(interpolation, 1)\n",
    "\n",
    "    # Export results to storage\n",
    "    out_key = os.path.join('tmp', data_field, tile_id, 'chunk_{}-{}'.format(block_x, block_y)) + '.tif'\n",
    "    with open('output', 'rb') as output_file:\n",
    "        storage.put_object(BUCKET, out_key, output_file)\n",
    "    \n",
    "    return out_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lithops serverless computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:58:43.408027Z",
     "start_time": "2021-04-13T16:58:42.454398Z"
    }
   },
   "outputs": [],
   "source": [
    "lith = lithops.FunctionExecutor(backend='k8s', storage='ceph', runtime=RUNTIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:39:28.155159Z",
     "start_time": "2021-04-13T14:39:28.150797Z"
    }
   },
   "outputs": [],
   "source": [
    "iterdata = [('ceph://{}/MDT/{}.tif'.format(BUCKET, tile), i, j) \n",
    "            for i in range(SPLITS) for j in range(SPLITS) for tile in tiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:39:28.850404Z",
     "start_time": "2021-04-13T14:39:28.812552Z"
    }
   },
   "outputs": [],
   "source": [
    "pprint(iterdata)\n",
    "print('Total functions: {} tiles * ({}^2) splits * 4 calculations = {}'.format(\n",
    "    len(tiles), SPLITS, len(iterdata) * 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T06:56:29.270356Z",
     "start_time": "2021-04-08T06:56:27.066344Z"
    }
   },
   "outputs": [],
   "source": [
    "fut = lith.map(radiation_interpolation, iterdata, extra_args=(SPLITS,), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T06:56:30.638547Z",
     "start_time": "2021-04-08T06:56:29.277666Z"
    }
   },
   "outputs": [],
   "source": [
    "fut = lith.map(map_interpolation, iterdata, extra_args=(SPLITS,'temp'), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T06:56:32.429261Z",
     "start_time": "2021-04-08T06:56:31.673596Z"
    }
   },
   "outputs": [],
   "source": [
    "fut = lith.map(map_interpolation, iterdata, extra_args=(SPLITS,'humi'), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T06:56:34.748102Z",
     "start_time": "2021-04-08T06:56:34.186500Z"
    }
   },
   "outputs": [],
   "source": [
    "fut = lith.map(map_interpolation, iterdata, extra_args=(SPLITS,'wind'), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:00:51.123143Z",
     "start_time": "2021-04-08T06:56:40.139182Z"
    }
   },
   "outputs": [],
   "source": [
    "out_chunks = lith.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPIs (Interpolation)\n",
    "\n",
    "[Skip KPI section](#(End-of-KPI-section---Interpolation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:02:05.040966Z",
     "start_time": "2021-04-08T07:02:04.321663Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.plot(dst=\"geospatial_usecase/plots/interpolation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:02:45.854968Z",
     "start_time": "2021-04-08T07:02:45.848629Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"geospatial_usecase/plots/interpolation_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:02:54.046898Z",
     "start_time": "2021-04-08T07:02:54.036980Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"geospatial_usecase/plots/interpolation_timeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of .tif files being processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:29.258771Z",
     "start_time": "2021-04-08T07:03:29.255051Z"
    }
   },
   "outputs": [],
   "source": [
    "mdts_gtiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total size accounting that files were repeatedly processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:33.021230Z",
     "start_time": "2021-04-08T07:03:32.728522Z"
    }
   },
   "outputs": [],
   "source": [
    "data_size = sum(obj[\"Size\"] for obj in cloud_storage.list_objects(BUCKET) if obj[\"Key\"] in mdts_gtiff)\n",
    "data_size *= 4  # Each file was processed 4 times\n",
    "\n",
    "print(f\"Data size: {data_size / 1024**2} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:37.464531Z",
     "start_time": "2021-04-08T07:03:37.441706Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:51.895064Z",
     "start_time": "2021-04-08T07:03:51.879123Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(lith.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:03:39.286038Z",
     "start_time": "2021-04-08T07:03:39.276118Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_interpolation = get_process_cost(lith)\n",
    "print(f\"The experiment cost ${cost_interpolation:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:05:08.953755Z",
     "start_time": "2021-04-08T07:05:08.931643Z"
    }
   },
   "outputs": [],
   "source": [
    "tstamps = set()\n",
    "for future in lith.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"tstamp\"):\n",
    "            tstamps.add(future.stats[key])\n",
    "            \n",
    "duration = max(tstamps) - min(tstamps)\n",
    "print(\"Duration: \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:05:11.960461Z",
     "start_time": "2021-04-08T07:05:11.953193Z"
    }
   },
   "outputs": [],
   "source": [
    "throughput_interpolation = data_size / duration  # Bytes/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:05:13.342171Z",
     "start_time": "2021-04-08T07:05:13.338300Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Throughput: {throughput_interpolation / 1024**2} MiB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we compare the execution speed of a sample process performed in last section, using different amounts of parallel workers, in order to test the scalability of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:39:50.231357Z",
     "start_time": "2021-04-13T14:39:50.225667Z"
    }
   },
   "outputs": [],
   "source": [
    "parallel_workers = [12, 24, 48, 72]\n",
    "experiment_duration = dict.fromkeys(parallel_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform experiment several times and save duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:41:52.721071Z",
     "start_time": "2021-04-13T14:40:15.651918Z"
    }
   },
   "outputs": [],
   "source": [
    "for option in parallel_workers:\n",
    "    lith = lithops.FunctionExecutor(\n",
    "        backend='k8s', \n",
    "        storage='ceph',\n",
    "        runtime=RUNTIME,\n",
    "        workers=option, # Tells lithops to work w/only this number of concurrent workers\n",
    "        log_level=\"DEBUG\"\n",
    "    )\n",
    "    fut = lith.map(\n",
    "        map_interpolation, iterdata, extra_args=(SPLITS,'temp'), runtime_memory=2048\n",
    "    )\n",
    "    lith.get_result()\n",
    "    \n",
    "    tstamps = set()\n",
    "    for future in lith.futures:\n",
    "        for key in future.stats.keys():\n",
    "            if key.endswith(\"tstamp\"):\n",
    "                tstamps.add(future.stats[key])\n",
    "    duration = max(tstamps) - min(tstamps)\n",
    "    experiment_duration[option] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T14:41:55.994144Z",
     "start_time": "2021-04-13T14:41:55.989057Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of per-worker performance relative to first experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot represents two lines:\n",
    "- **Ideal speedup**: theoretical best speedup - scenario where a 2x increment in workers results in 1/2 execution time, a 4x increment in workers results in a 1/4 execution time, etc.\n",
    "- **Lithops speedup**: actual speedup that results from the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T15:09:52.888690Z",
     "start_time": "2021-04-13T15:09:52.723516Z"
    }
   },
   "outputs": [],
   "source": [
    "duration = list(experiment_duration.values())\n",
    "theoretical_best_speedup = [(1 - parallel_workers[0] / parallel_workers[i]) * 100 for i in range(0, len(parallel_workers))]\n",
    "actual_speedup = [(1 - duration[i] / duration[0]) * 100 for i in range(0, len(duration))]\n",
    "\n",
    "plt.plot(\n",
    "    parallel_workers,\n",
    "    theoretical_best_speedup\n",
    ")\n",
    "plt.plot(\n",
    "    parallel_workers,\n",
    "    actual_speedup\n",
    ")\n",
    "plt.xlabel(\"Number of workers\")\n",
    "plt.ylabel(\"% time reduced, relative to first experiment\")\n",
    "plt.legend([\"Ideal speedup\", \"Lithops speedup (this experiment)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:11:22.584754Z",
     "start_time": "2021-04-08T07:11:22.577217Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.futures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:10:54.929461Z",
     "start_time": "2021-04-08T07:10:54.917364Z"
    }
   },
   "source": [
    "###### (End of KPI section - Interpolation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join split subsets into a tile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:21:21.921233Z",
     "start_time": "2021-04-08T07:21:21.911497Z"
    }
   },
   "outputs": [],
   "source": [
    "def gather_blocks(tile, splits, data_field, storage):\n",
    "\n",
    "    from rasterio.windows import Window\n",
    "    \n",
    "    # Get width and height from original tile\n",
    "    with rasterio.open(storage.get_object(bucket=BUCKET, key=f'MDT/{tile}.tif', stream=True)) as og:\n",
    "        height = og.profile['height']\n",
    "        width = og.profile['width']\n",
    "    \n",
    "    chunk_tiles = storage.list_keys(bucket=BUCKET, prefix=f'tmp/{data_field}/{tile}/chunk')\n",
    "        \n",
    "    # Open first object to obtain profile metadata\n",
    "    with rasterio.open(storage.get_object(bucket=BUCKET, key=chunk_tiles[0], stream=True)) as src:\n",
    "        profile = src.profile\n",
    "        profile.update(width=width)\n",
    "        profile.update(height=height)\n",
    "\n",
    "    # Iterate each object and print its block into the destination file\n",
    "    with rasterio.open(\"output\", \"w\", **profile) as dest: \n",
    "        for chunk in chunk_tiles:\n",
    "            j, i = os.path.splitext(os.path.basename(chunk))[0].rsplit('_')[1].split('-')\n",
    "            j, i = int(j), int(i)\n",
    "            with rasterio.open(storage.get_object(bucket=BUCKET, key=chunk, stream=True)) as src:\n",
    "                step_w = math.floor(width / splits)\n",
    "                step_h = math.floor(height / splits)\n",
    "                curr_window = Window(round(step_w * i), round(step_h * j), src.width, src.height)\n",
    "                content = src.read(1)\n",
    "                dest.write(content, 1, window=curr_window)\n",
    "            # storage.delete_object(bucket=BUCKET, key=chunk)\n",
    "    \n",
    "    output_key = os.path.join('tmp', data_field, tile, '_'.join([tile, data_field.upper()+'.tif']))\n",
    "    with open('output', 'rb') as out_file:\n",
    "        storage.put_object(bucket=BUCKET, key=output_key, body=out_file)  \n",
    "    \n",
    "    return output_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine previous split subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:21:43.580625Z",
     "start_time": "2021-04-08T07:21:43.180508Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'extrad'), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:21:44.784961Z",
     "start_time": "2021-04-08T07:21:44.401457Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'humi'), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:21:45.855437Z",
     "start_time": "2021-04-08T07:21:45.488359Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'rad'), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:21:46.912489Z",
     "start_time": "2021-04-08T07:21:46.515196Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'temp'), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:21:48.282075Z",
     "start_time": "2021-04-08T07:21:47.908570Z"
    }
   },
   "outputs": [],
   "source": [
    "fut = lith.map(gather_blocks, tiles, extra_args=(SPLITS, 'wind'), runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:22:14.948724Z",
     "start_time": "2021-04-08T07:21:52.356798Z"
    }
   },
   "outputs": [],
   "source": [
    "out_combined = lith.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPIs (Gather blocks)\n",
    "\n",
    "[Skip KPI section](#(End-of-KPI-section---Gather-blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:23:42.594365Z",
     "start_time": "2021-04-08T07:23:42.135958Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.plot(dst=\"geospatial_usecase/plots/gather_blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:23:44.168346Z",
     "start_time": "2021-04-08T07:23:44.162948Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"geospatial_usecase/plots/gather_blocks_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T07:23:45.856957Z",
     "start_time": "2021-04-08T07:23:45.840851Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"geospatial_usecase/plots/gather_blocks_timeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:17:25.463278Z",
     "start_time": "2021-04-08T08:17:25.458630Z"
    }
   },
   "outputs": [],
   "source": [
    "mdts_gtiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:17:30.709533Z",
     "start_time": "2021-04-08T08:17:29.875330Z"
    }
   },
   "outputs": [],
   "source": [
    "data_size = sum(obj[\"Size\"] for obj in cloud_storage.list_objects(BUCKET) if obj[\"Key\"] in mdts_gtiff)\n",
    "data_size *= 4  # Each file was processed 4 times\n",
    "\n",
    "print(f\"Data size: {data_size / 1024**2} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:18:54.513927Z",
     "start_time": "2021-04-08T08:18:54.486157Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:19:02.264749Z",
     "start_time": "2021-04-08T08:19:02.236457Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(lith.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:21:09.051314Z",
     "start_time": "2021-04-08T08:21:09.031537Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_gather_blocks = get_process_cost(lith)\n",
    "print(f\"The experiment cost ${cost_gather_blocks:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:19:57.208334Z",
     "start_time": "2021-04-08T08:19:57.189780Z"
    }
   },
   "outputs": [],
   "source": [
    "tstamps = set()\n",
    "for future in lith.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"tstamp\"):\n",
    "            tstamps.add(future.stats[key])\n",
    "            \n",
    "duration = max(tstamps) - min(tstamps)\n",
    "print(\"Duration: \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:21:20.407233Z",
     "start_time": "2021-04-08T08:21:20.404210Z"
    }
   },
   "outputs": [],
   "source": [
    "throughput_gather_blocks = data_size / duration  # Bytes/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:21:35.726076Z",
     "start_time": "2021-04-08T08:21:35.720504Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Throughput: {throughput_gather_blocks / 1024**2} MiB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-08T08:22:57.849484Z",
     "start_time": "2021-04-08T08:22:57.844112Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.futures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (End of KPI section - Gather blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of potential evaporation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:51.493674Z",
     "start_time": "2021-04-13T16:57:51.485032Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_crop_evapotranspiration(temperatures,\n",
    "                                    humidities,\n",
    "                                    wind_speeds,\n",
    "                                    external_radiations,\n",
    "                                    global_radiations,\n",
    "                                    KCs):\n",
    "    gamma = 0.665*101.3/1000\n",
    "    eSat = 0.6108 * np.exp((17.27*temperatures)/(temperatures+237.3))\n",
    "    delta = 4098 * eSat / np.power((temperatures + 237.3),2)\n",
    "    eA = np.where(humidities < 0, 0, eSat * humidities / 100)     # Avoid sqrt of a negative number\n",
    "    T4 = 4.903 * np.power((273.3 + temperatures),4)/1000000000\n",
    "    rSrS0 = global_radiations/(external_radiations * 0.75)\n",
    "    rN = 0.8* global_radiations-T4*(0.34-0.14*np.sqrt(eA))*((1.35*rSrS0)-0.35)\n",
    "    den = delta + gamma *(1 + 0.34* wind_speeds)\n",
    "    tRad = 0.408 * delta * rN / den\n",
    "    tAdv = gamma * (900/(temperatures+273))*wind_speeds * (eSat - eA)/den\n",
    "    return ((tRad + tAdv) * 7 * KCs).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:52.760441Z",
     "start_time": "2021-04-13T16:57:52.753812Z"
    }
   },
   "outputs": [],
   "source": [
    "vineyard = ['VI', 'VO', 'VF', 'FV', 'CV' ]\n",
    "olive_grove = ['OV', 'VO', 'OF', 'FL', 'OC']\n",
    "fruit = ['FY', 'VF', 'OF', 'FF', 'CF']\n",
    "nuts = ['FS', 'FV', 'FL', 'FF', 'CS' ]\n",
    "citrus = ['CI', 'CV', 'OC', 'CF', 'CS' ]\n",
    "\n",
    "def get_kc(feature):\n",
    "    \n",
    "    # TODO: Get more precise values of Kc\n",
    "    sigpac_use = feature['properties']['uso_sigpac']\n",
    "    if sigpac_use in vineyard:\n",
    "        # Grapes for wine - 0.3, 0.7, 0.45\n",
    "        return 0.7  \n",
    "    if sigpac_use in olive_grove:\n",
    "        # Olive grove - ini: 0.65, med: 0.7, end: 0.7\n",
    "        return 0.7 \n",
    "    if sigpac_use in fruit:\n",
    "        # Apples, Cherries, Pears - 0.45, 0.95, 0.7\n",
    "        return 0.95\n",
    "    if sigpac_use in nuts:\n",
    "        # Almonds - 0.4, 0.9, 0.65\n",
    "        return 0.9\n",
    "    if sigpac_use in citrus:\n",
    "        # Citrus, without ground coverage - 0.7, 0.65, 0.7\n",
    "        return 0.65\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:54.250115Z",
     "start_time": "2021-04-13T16:57:54.243932Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_geometry_window(src, geom_bounds):\n",
    "    left, bottom, right, top = geom_bounds\n",
    "    src_left, src_bottom, src_right, src_top = src.bounds\n",
    "    window = src.window(max(left,src_left), max(bottom,src_bottom), min(right,src_right), min(top,src_top))\n",
    "    window_floored = window.round_offsets(op='floor', pixel_precision=3)\n",
    "    w = math.ceil(window.width + window.col_off - window_floored.col_off)\n",
    "    h = math.ceil(window.height + window.row_off - window_floored.row_off)\n",
    "    return Window(window_floored.col_off, window_floored.row_off, w, h)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:57.781920Z",
     "start_time": "2021-04-13T16:57:57.770029Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_evapotranspiration_by_shape(tem, hum, win, rad, extrad, dst):\n",
    "    \n",
    "    import fiona\n",
    "    from shapely.geometry import shape, box\n",
    "    from rasterio import features\n",
    "    \n",
    "    non_arable_land = ['AG', 'CA', 'ED', 'FO', 'IM', 'PA', 'PR', 'ZU', 'ZV']\n",
    "    \n",
    "    with fiona.open('zip://shape.zip') as shape_src:\n",
    "        for feature in shape_src.filter(bbox=tem.bounds):\n",
    "            KC = get_kc(feature) \n",
    "            if KC is not None:   \n",
    "                geom = shape(feature['geometry'])  \n",
    "                window = get_geometry_window(tem, geom.bounds)              \n",
    "                win_transform = rasterio.windows.transform(window, tem.transform)\n",
    "                # Convert shape to raster matrix\n",
    "                image = features.rasterize([geom],\n",
    "                                           out_shape=(window.height, window.width),\n",
    "                                           transform = win_transform,\n",
    "                                           fill = 0,\n",
    "                                           default_value = 1).astype('bool')\n",
    "                # Get values to compute evapotranspiration\n",
    "                temperatures = tem.read(1, window=window)\n",
    "                humidities = hum.read(1, window=window)\n",
    "                wind_speeds = win.read(1, window=window)\n",
    "                # Convert from W to MJ (0.0036)\n",
    "                global_radiations = rad.read(1, window=window) * 0.0036\n",
    "                external_radiations = extrad.read(1, window=window) * 0.0036\n",
    "                KCs = np.full(temperatures.shape, KC)\n",
    "                # TODO: compute external radiation\n",
    "                #external_radiations = np.full(temperatures.shape, 14)\n",
    "                # TODO: compute global radiation\n",
    "                # global_radiations = np.full(temperatures.shape, 10)\n",
    "                etc = compute_crop_evapotranspiration(\n",
    "                        temperatures,\n",
    "                        humidities,\n",
    "                        wind_speeds,\n",
    "                        external_radiations,\n",
    "                        global_radiations,\n",
    "                        KCs\n",
    "                )\n",
    "                etc[temperatures == tem.nodata] = dst.nodata\n",
    "                etc[np.logical_not(image)] = dst.nodata\n",
    "                dst.write(etc + dst.read(1, window=window), 1, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:57:59.216824Z",
     "start_time": "2021-04-13T16:57:59.207435Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_global_evapotranspiration(tem, hum, win, rad, extrad, dst):    \n",
    "    for ji, window in tem.block_windows(1):\n",
    "        bounds = rasterio.windows.bounds(window, tem.transform)\n",
    "        temperatures = tem.read(1, window=window)\n",
    "        humidities = hum.read(1, window=window)\n",
    "        wind_speeds = win.read(1, window=window)\n",
    "         # Convert from W to MJ (0.0036)\n",
    "        global_radiations = rad.read(1, window=window) * 0.0036\n",
    "        external_radiations = extrad.read(1, window=window) * 0.0036\n",
    "        # TODO: compute external radiation\n",
    "        #external_radiations = np.full(temperatures.shape, 14)\n",
    "        # TODO: compute global radiation\n",
    "        # global_radiations = np.full(temperatures.shape, 10)\n",
    "        # TODO: compute KCs\n",
    "        KCs = np.full(temperatures.shape, 1)\n",
    "        etc = compute_crop_evapotranspiration(\n",
    "                temperatures,\n",
    "                humidities,\n",
    "                wind_speeds,\n",
    "                external_radiations,\n",
    "                global_radiations,\n",
    "                KCs\n",
    "        )\n",
    "        dst.write(np.where(temperatures == tem.nodata, dst.nodata, etc), 1, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:58:01.128416Z",
     "start_time": "2021-04-13T16:58:01.101842Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_calculations(tile, storage):\n",
    "    \n",
    "    from functools import partial\n",
    "      \n",
    "    # Download shapefile\n",
    "    shapefile = storage.get_object(bucket=BUCKET, key='shapefile.zip', stream=True)\n",
    "    with open('shape.zip', 'wb') as shapf:\n",
    "        for chunk in iter(partial(shapefile.read, 200 * 1024 * 1024), ''):\n",
    "            if not chunk:\n",
    "                break\n",
    "            shapf.write(chunk)\n",
    "    \n",
    "    temp = storage.get_object(bucket=BUCKET, key=f'tmp/temp/{tile}/{tile}_TEMP.tif', stream=True)\n",
    "    humi = storage.get_object(bucket=BUCKET, key=f'tmp/humi/{tile}/{tile}_HUMI.tif', stream=True)\n",
    "    rad = storage.get_object(bucket=BUCKET, key=f'tmp/rad/{tile}/{tile}_RAD.tif', stream=True)\n",
    "    extrad = storage.get_object(bucket=BUCKET, key=f'tmp/extrad/{tile}/{tile}_EXTRAD.tif', stream=True)\n",
    "    wind = storage.get_object(bucket=BUCKET, key=f'tmp/wind/{tile}/{tile}_WIND.tif', stream=True)\n",
    "    \n",
    "    with rasterio.open(temp) as temp_raster:\n",
    "        with rasterio.open(humi) as humi_raster:\n",
    "            with rasterio.open(rad) as rad_raster:\n",
    "                with rasterio.open(extrad) as extrad_raster:\n",
    "                    with rasterio.open(wind) as wind_raster:\n",
    "                        profile = temp_raster.profile\n",
    "                        profile.update(nodata=0)\n",
    "        \n",
    "                        with rasterio.open('output', 'w+', **profile) as dst:\n",
    "#                             compute_global_evapotranspiration(temp_raster, humi_raster, wind_raster,\n",
    "#                                                               rad_raster, extrad_raster, dst)\n",
    "                            compute_evapotranspiration_by_shape(temp_raster, humi_raster, wind_raster,\n",
    "                                                                rad_raster, extrad_raster, dst)\n",
    "    \n",
    "    out_key = f'etc/{tile}_ETC.tif'\n",
    "    with open('output', 'rb') as output_f:\n",
    "        storage.put_object(bucket=BUCKET, key=out_key, body=output_f)\n",
    "    return out_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T16:58:55.822484Z",
     "start_time": "2021-04-13T16:58:54.943303Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fut = lith.map(combine_calculations, tiles, runtime_memory=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T17:01:57.663322Z",
     "start_time": "2021-04-13T16:58:57.632356Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = lith.get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPIs (Potential evaporation)\n",
    "\n",
    "[Skip KPI section](#(End-of-KPI-section---Potential-evaporation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:06:46.567960Z",
     "start_time": "2021-04-13T20:06:45.870566Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.plot(dst=\"geospatial_usecase/plots/potential_evaporation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:04.735119Z",
     "start_time": "2021-04-13T20:07:04.729026Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"geospatial_usecase/plots/potential_evaporation_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:08.400698Z",
     "start_time": "2021-04-13T20:07:08.395328Z"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"geospatial_usecase/plots/potential_evaporation_timeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:08:12.027411Z",
     "start_time": "2021-04-13T20:08:12.018635Z"
    }
   },
   "outputs": [],
   "source": [
    "tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:54:33.848670Z",
     "start_time": "2021-04-13T20:54:33.645465Z"
    }
   },
   "outputs": [],
   "source": [
    "data_size = 0\n",
    "\n",
    "for obj in cloud_storage.list_objects(BUCKET):\n",
    "    for tile in tiles:\n",
    "        if obj[\"Key\"] == f'tmp/temp/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == f'tmp/humi/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == f'tmp/rad/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == f'tmp/extrad/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == f'tmp/wind/{tile}/{tile}_TEMP.tif' or \\\n",
    "                obj[\"Key\"] == 'shapefile.zip':\n",
    "            data_size += obj[\"Size\"]\n",
    "\n",
    "print(f\"Data size: {data_size / 1024**2} MiB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:54:45.609853Z",
     "start_time": "2021-04-13T20:54:45.592066Z"
    }
   },
   "outputs": [],
   "source": [
    "lith.job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:16.935146Z",
     "start_time": "2021-04-13T20:07:16.918524Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(lith.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:07:26.235760Z",
     "start_time": "2021-04-13T20:07:26.226071Z"
    }
   },
   "outputs": [],
   "source": [
    "cost_potential_evaporation = get_process_cost(lith)\n",
    "print(f\"The experiment cost ${cost_potential_evaporation:.4f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:55:04.595689Z",
     "start_time": "2021-04-13T20:55:04.581005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tstamps = set()\n",
    "for future in lith.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"tstamp\"):\n",
    "            tstamps.add(future.stats[key])\n",
    "            \n",
    "duration = max(tstamps) - min(tstamps)\n",
    "print(\"Duration: \" + str(duration) + \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:55:07.194755Z",
     "start_time": "2021-04-13T20:55:07.192029Z"
    }
   },
   "outputs": [],
   "source": [
    "throughput_potential_evaporation = data_size / duration  # Bytes/second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T20:55:09.220719Z",
     "start_time": "2021-04-13T20:55:09.212358Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Throughput: {throughput_potential_evaporation / 1024**2} MiB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KPI: Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we compare the execution speed of a sample process performed in last section, using different amounts of parallel workers, in order to test the scalability of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:10:15.560833Z",
     "start_time": "2021-04-13T21:10:15.556295Z"
    }
   },
   "outputs": [],
   "source": [
    "parallel_workers = [2, 4, 8]\n",
    "experiment_duration = dict.fromkeys(parallel_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform experiment several times and save duration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:26:47.958535Z",
     "start_time": "2021-04-13T21:10:17.409017Z"
    }
   },
   "outputs": [],
   "source": [
    "for option in parallel_workers:\n",
    "    lith = lithops.FunctionExecutor(\n",
    "        backend='k8s', \n",
    "        storage='ceph', \n",
    "        runtime=RUNTIME,\n",
    "        workers=option, # Tells lithops to work w/only this number of concurrent workers\n",
    "        log_level=\"NOTSET\"\n",
    "    )\n",
    "    lith.map(combine_calculations, tiles, runtime_memory=2048)\n",
    "    lith.get_result()\n",
    "    \n",
    "    tstamps = set()\n",
    "    for future in lith.futures:\n",
    "        for key in future.stats.keys():\n",
    "            if key.endswith(\"tstamp\"):\n",
    "                tstamps.add(future.stats[key])\n",
    "    duration = max(tstamps) - min(tstamps)\n",
    "    experiment_duration[option] = duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:29:20.900597Z",
     "start_time": "2021-04-13T21:29:20.893752Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization of per-worker performance relative to first experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following plot represents two lines:\n",
    "- **Ideal speedup**: theoretical best speedup - scenario where a 2x increment in workers results in 1/2 execution time, a 4x increment in workers results in a 1/4 execution time, etc.\n",
    "- **Lithops speedup**: actual speedup that results from the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:29:33.018923Z",
     "start_time": "2021-04-13T21:29:32.780739Z"
    }
   },
   "outputs": [],
   "source": [
    "duration = list(experiment_duration.values())\n",
    "theoretical_best_speedup = [(1 - parallel_workers[0] / parallel_workers[i]) * 100 for i in range(0, len(parallel_workers))]\n",
    "actual_speedup = [(1 - duration[i] / duration[0]) * 100 for i in range(0, len(duration))]\n",
    "\n",
    "plt.plot(\n",
    "    parallel_workers,\n",
    "    theoretical_best_speedup\n",
    ")\n",
    "plt.plot(\n",
    "    parallel_workers,\n",
    "    actual_speedup\n",
    ")\n",
    "plt.xlabel(\"Number of workers\")\n",
    "plt.ylabel(\"% time reduced, relative to first experiment\")\n",
    "plt.legend([\"Ideal speedup\", \"Lithops speedup (this experiment)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lith.futures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### (End of KPI section - Potential evaporation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:30:27.539704Z",
     "start_time": "2021-04-13T21:30:26.276005Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "tile = random.choice(tiles)\n",
    "obj = io.BytesIO(cloud_storage.get_object(bucket=BUCKET, key=f'etc/{tile}_ETC.tif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-13T21:32:01.429761Z",
     "start_time": "2021-04-13T21:32:00.245568Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "with rasterio.open(obj) as src:\n",
    "    arr = src.read(1, out_shape=(src.height, src.width))\n",
    "    ax.set_title(tile)\n",
    "    img = ax.imshow(arr, cmap='Greens')\n",
    "    fig.colorbar(img, shrink=0.5)\n",
    "\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n",
    "\n",
    "obj.seek(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove intermediate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T10:06:07.571233Z",
     "start_time": "2021-03-29T10:05:14.526Z"
    }
   },
   "outputs": [],
   "source": [
    "# keys = cloud_storage.list_keys(bucket=BUCKET, prefix='')\n",
    "# keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-29T10:06:07.574068Z",
     "start_time": "2021-03-29T10:05:14.528Z"
    }
   },
   "outputs": [],
   "source": [
    "# for key in keys:\n",
    "#     cloud_storage.delete_object(bucket=BUCKET, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
